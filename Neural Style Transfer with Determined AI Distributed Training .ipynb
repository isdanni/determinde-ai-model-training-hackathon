{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ab19e65",
   "metadata": {},
   "source": [
    "# Neural Style Transfer with Determined AI Distributed Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c1a62d",
   "metadata": {},
   "source": [
    "In this notebook, we are using [Determined AI Distributed Training Platform](https://www.googleadservices.com/pagead/aclk?sa=L&ai=DChcSEwj0qJ2Hha3-AhXKMq0GHdKjAHsYABAAGgJwdg&ohost=www.google.com&cid=CAESauD28j3TAEQF3m2XI5muKYYFzyP5j_nYonVGVJg5j0l7ImbzKbJzE3317fwr9tHoies3u_WbAXhOvYKWOS-uhOn1TfKKWuaELUEb01YWMkU23PIzqaQO0Rc4vj4ycnsAEANVyXvnv9pTioY&sig=AOD64_2oY2aUJkycrKR4Tq71uJOV_rGjDQ&q&adurl&ved=2ahUKEwjQ9JSHha3-AhXjOX0KHSLcB8gQ0Qx6BAgFEAE) to train a customized neural style transfer CNN Model built with PyTorch to obtain style transfer results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78979561",
   "metadata": {},
   "source": [
    "## 1. Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeeb60cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: determined in /home/danni/Documents/code/venv/lib/python3.8/site-packages (0.21.1)\n",
      "Requirement already satisfied: matplotlib in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (3.7.1)\n",
      "Requirement already satisfied: packaging in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (23.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (1.24.2)\n",
      "Requirement already satisfied: psutil in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (5.9.4)\n",
      "Requirement already satisfied: pyzmq>=18.1.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (25.0.2)\n",
      "Requirement already satisfied: yogadl==0.1.4 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (0.1.4)\n",
      "Requirement already satisfied: backoff in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (1.10.0)\n",
      "Requirement already satisfied: certifi in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (2022.12.7)\n",
      "Requirement already satisfied: filelock in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (3.11.0)\n",
      "Requirement already satisfied: google-cloud-storage in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (2.8.0)\n",
      "Requirement already satisfied: hdfs>=2.2.2 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (2.7.0)\n",
      "Requirement already satisfied: lomond>=0.3.3 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (0.3.3)\n",
      "Requirement already satisfied: pathspec>=0.6.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (0.11.1)\n",
      "Requirement already satisfied: azure-core in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (1.26.4)\n",
      "Requirement already satisfied: azure-storage-blob in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (12.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (2.2.0)\n",
      "Requirement already satisfied: boto3 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (1.26.114)\n",
      "Requirement already satisfied: argcomplete>=1.9.4 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (3.0.5)\n",
      "Requirement already satisfied: gitpython>=3.1.3 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (3.1.31)\n",
      "Requirement already satisfied: pyOpenSSL>=19.1.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (23.1.1)\n",
      "Requirement already satisfied: python-dateutil in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (2.8.2)\n",
      "Requirement already satisfied: pytz in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (2023.3)\n",
      "Requirement already satisfied: tabulate>=0.8.3 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (0.9.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.15.29 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (0.17.21)\n",
      "Requirement already satisfied: docker[ssh]>=3.7.3 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (6.0.1)\n",
      "Requirement already satisfied: google-api-python-client>=1.12.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (2.85.0)\n",
      "Requirement already satisfied: paramiko>=2.4.2 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (3.1.0)\n",
      "Requirement already satisfied: docker-compose>=1.13.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (1.29.2)\n",
      "Requirement already satisfied: tqdm in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (4.65.0)\n",
      "Requirement already satisfied: appdirs in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (1.4.4)\n",
      "Requirement already satisfied: websocket-client<1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (0.59.0)\n",
      "Requirement already satisfied: analytics-python in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (1.4.post1)\n",
      "Requirement already satisfied: async-generator in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from yogadl==0.1.4->determined) (1.10)\n",
      "Requirement already satisfied: lmdb in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from yogadl==0.1.4->determined) (1.4.1)\n",
      "Requirement already satisfied: websockets>=8.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from yogadl==0.1.4->determined) (11.0.1)\n",
      "Requirement already satisfied: PyYAML<6,>=3.10 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from docker-compose>=1.13.0->determined) (5.4.1)\n",
      "Requirement already satisfied: distro<2,>=1.5.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from docker-compose>=1.13.0->determined) (1.8.0)\n",
      "Requirement already satisfied: dockerpty<1,>=0.4.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from docker-compose>=1.13.0->determined) (0.4.1)\n",
      "Requirement already satisfied: docopt<1,>=0.6.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from docker-compose>=1.13.0->determined) (0.6.2)\n",
      "Requirement already satisfied: jsonschema<4,>=2.5.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from docker-compose>=1.13.0->determined) (3.2.0)\n",
      "Requirement already satisfied: python-dotenv<1,>=0.13.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from docker-compose>=1.13.0->determined) (0.21.1)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from docker-compose>=1.13.0->determined) (2.28.2)\n",
      "Requirement already satisfied: texttable<2,>=0.9.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from docker-compose>=1.13.0->determined) (1.6.7)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from docker[ssh]>=3.7.3->determined) (1.26.15)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from gitpython>=3.1.3->determined) (4.0.10)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-api-python-client>=1.12.1->determined) (0.22.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-api-python-client>=1.12.1->determined) (2.17.3)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-api-python-client>=1.12.1->determined) (0.1.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-api-python-client>=1.12.1->determined) (2.11.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-api-python-client>=1.12.1->determined) (4.1.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from hdfs>=2.2.2->determined) (1.16.0)\n",
      "Requirement already satisfied: bcrypt>=3.2 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from paramiko>=2.4.2->determined) (4.0.1)\n",
      "Requirement already satisfied: cryptography>=3.3 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from paramiko>=2.4.2->determined) (40.0.2)\n",
      "Requirement already satisfied: pynacl>=1.5 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from paramiko>=2.4.2->determined) (1.5.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.6 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from ruamel.yaml>=0.15.29->determined) (0.2.7)\n",
      "Requirement already satisfied: monotonic>=1.5 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from analytics-python->determined) (1.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from azure-core->determined) (4.5.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from azure-storage-blob->determined) (0.6.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.114 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from boto3->determined) (1.29.114)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from boto3->determined) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from boto3->determined) (0.6.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-cloud-storage->determined) (2.3.2)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-cloud-storage->determined) (2.4.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from matplotlib->determined) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from matplotlib->determined) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from matplotlib->determined) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from matplotlib->determined) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from matplotlib->determined) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from matplotlib->determined) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from matplotlib->determined) (5.12.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from cryptography>=3.3->paramiko>=2.4.2->determined) (1.15.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.3->determined) (5.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.12.1->determined) (1.59.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.12.1->determined) (4.22.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.12.1->determined) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.12.1->determined) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.12.1->determined) (4.9)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-resumable-media>=2.3.2->google-cloud-storage->determined) (1.5.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->determined) (3.15.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from jsonschema<4,>=2.5.1->docker-compose>=1.13.0->determined) (22.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from jsonschema<4,>=2.5.1->docker-compose>=1.13.0->determined) (0.19.3)\n",
      "Requirement already satisfied: setuptools in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from jsonschema<4,>=2.5.1->docker-compose>=1.13.0->determined) (67.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from requests<3,>=2.20.0->docker-compose>=1.13.0->determined) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from requests<3,>=2.20.0->docker-compose>=1.13.0->determined) (3.4)\n",
      "Requirement already satisfied: pycparser in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4.2->determined) (2.21)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.12.1->determined) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install determined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "688f2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio\n",
    "# commented out to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b799d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50159026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33d3162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# turn off warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a14b66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd9192",
   "metadata": {},
   "source": [
    "## 2. Problem Definition & Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b30689",
   "metadata": {},
   "source": [
    "Imganie creating a copy of a painting as a mathematical optimization problem. Given the source image `S`, we want to generate a target image `T`, so that the mean square error **MSE(S-T)** is the smallest, which means this imaage generation problem becomes an optimization problem of finding the optimal `T`.\n",
    "\n",
    "For this problem, we can randomly initialize an image `T`, perform gradient descent based on this optimization goal, and eventually find the optimal `T`, a target image that is the closest to the source image `S`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9699be7b",
   "metadata": {},
   "source": [
    "![model-architecture](images/model-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a04689",
   "metadata": {},
   "source": [
    "paper link: [Image Style Transfer Using Convolutional Neural Networks](https://openaccess.thecvf.com/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf) \n",
    "\n",
    "The expected result should be:\n",
    "\n",
    "![paper image](images/paper-image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f80ad",
   "metadata": {},
   "source": [
    "## 3. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481b7c7",
   "metadata": {},
   "source": [
    "In order to calculate MSE correctly, the shape of all images must be **uniform**. Therefore, we resize images to `(256, 256)`. \n",
    "\n",
    "After data preprocessing, the image format is in `(c, h, w)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae9a900",
   "metadata": {},
   "source": [
    "Style Image            |  Content Image\n",
    ":-------------------------:|:-------------------------:\n",
    "![](images/andyw.jpg)  |  ![](images/grogu.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eff22088",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d950313",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (256, 256)\n",
    "\n",
    "\n",
    "def read_image(image_path):\n",
    "    pipeline = transforms.Compose([transforms.Resize((img_size)), transforms.ToTensor()])\n",
    "\n",
    "    img = Image.open(image_path)\n",
    "    img = pipeline(img).unsqueeze(0)\n",
    "    return img.to(device, torch.float)\n",
    "\n",
    "\n",
    "def save_image(tensor, image_path):\n",
    "    toPIL = transforms.ToPILImage()\n",
    "    \n",
    "    img = tensor.detach().cpu().clone()\n",
    "    img = toPIL(img.squeeze(0))\n",
    "    \n",
    "    img.save(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72801f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_img = read_image('images/andyw.jpg')\n",
    "content_img = read_image('images/grogu.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fc61835",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = torch.randn(1, 3, *img_size, device=device)\n",
    "input_img.requires_grad_(True)\n",
    "optimizer = optim.LBFGS([input_img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a48ae162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "Loss: 2.5425872802734375\n",
      "Step 2:\n",
      "Loss: 2.5425591468811035\n",
      "Step 3:\n",
      "Loss: 0.11352577805519104\n",
      "Step 4:\n",
      "Loss: 0.11331774294376373\n",
      "Step 5:\n",
      "Loss: 0.11331774294376373\n",
      "Step 6:\n",
      "Loss: 0.11331774294376373\n",
      "Step 7:\n",
      "Loss: 0.11331774294376373\n",
      "Step 8:\n",
      "Loss: 0.11331774294376373\n",
      "Step 9:\n",
      "Loss: 0.11331774294376373\n",
      "Step 10:\n",
      "Loss: 0.11331774294376373\n",
      "Step 11:\n",
      "Loss: 0.11331774294376373\n"
     ]
    }
   ],
   "source": [
    "steps = 0\n",
    "while steps <= 10:\n",
    "    def closure():\n",
    "        global steps\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.mse_loss(input_img, style_img) + F.mse_loss(input_img, content_img)\n",
    "        loss.backward()\n",
    "        \n",
    "        steps += 1\n",
    "        if steps % 1 == 0:\n",
    "            print(f'Step {steps}:')\n",
    "            print(f'Loss: {loss}')\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "\n",
    "save_image(input_img, 'images/output.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1eedea",
   "metadata": {},
   "source": [
    "The output picture now looks like this: ![out](images/output.jpg). We will be reusing `read_image` and `save_image` in the following model training tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6654e91",
   "metadata": {},
   "source": [
    "## 4. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157f3e5",
   "metadata": {},
   "source": [
    "### 4.1. Loss Function\n",
    "Since we are using MSE to cauculate loss in this notebook, we will reuse its `F.mse_loss` directly instead of defining our own `forward()` functions in the `torch.autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05f74ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(x: torch.Tensor):\n",
    "    n, c, h, w = x.shape\n",
    "\n",
    "    features = x.reshape(n * c, h * w)\n",
    "    features = torch.mm(features, features.T) / n / c / h / w\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67406ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(torch.nn.Module):\n",
    "    def __init__(self, target: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input\n",
    "    \n",
    "\n",
    "class StyleLoss(torch.nn.Module):\n",
    "    def __init__(self, target: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.target = features(target.detach()).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = features(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4073a75",
   "metadata": {},
   "source": [
    "### 4.2. Normalization\n",
    "\n",
    "We also use one layer to normalize input distribution so the mean and standard deviation of the input data can be directly used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61b9d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization(torch.nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        self.mean = torch.tensor(mean).to(device).reshape(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).to(device).reshape(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a8bb2b",
   "metadata": {},
   "source": [
    "### 4.3. Model Assemble \n",
    "\n",
    "Following, we assemble the norm layer, the loss calculation and features(ONLY) of a pretrained VGG model from `torch.vision` using `torch.nn.Sequential`. If a certain layer is used for MSE calculation, we attach the loss module to it. \n",
    "\n",
    "In the following log, we can see the model architecture consists of:\n",
    "\n",
    "```\n",
    " norm => conv_1 => style_loss_1 => relu_1 => conv_2 => style_loss_2 => relu_2 => pool_2 => conv_3 => style_loss_3 => relu_3 => conv_4 => content_loss_4 => style_loss_4 => relu_4 => pool_4 => conv_5 => style_loss_5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23c7abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_content_layers = ['conv_4']\n",
    "default_style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "def get_model_and_losses(content_img, style_img, content_layers, style_layers):\n",
    "    num_loss = 0\n",
    "    expected_num_loss = len(content_layers) + len(style_layers)\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    model = torch.nn.Sequential(\n",
    "        Normalization([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]))\n",
    "    cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "    i = 0\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, torch.nn.Conv2d):\n",
    "            i += 1\n",
    "            name = f'conv_{i}'\n",
    "        elif isinstance(layer, torch.nn.ReLU):\n",
    "            name = f'relu_{i}'\n",
    "            layer = torch.nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, torch.nn.MaxPool2d):\n",
    "            name = f'pool_{i}'\n",
    "        elif isinstance(layer, torch.nn.BatchNorm2d):\n",
    "            name = f'bn_{i}'\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f'Unrecognized layer: {layer.__class__.__name__}')\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            target = model(content_img)\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(f'content_loss_{i}', content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "            num_loss += 1\n",
    "\n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_img)\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(f'style_loss_{i}', style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "            num_loss += 1\n",
    "\n",
    "        if num_loss >= expected_num_loss:\n",
    "            break\n",
    "\n",
    "    return model, content_losses, style_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3caaaf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_img = read_image('images/andyw.jpg')\n",
    "content_img = read_image('images/grogu.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ec180ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Normalization()\n",
       "  (conv_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (style_loss_1): StyleLoss()\n",
       "  (relu_1): ReLU()\n",
       "  (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (style_loss_2): StyleLoss()\n",
       "  (relu_2): ReLU()\n",
       "  (pool_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (style_loss_3): StyleLoss()\n",
       "  (relu_3): ReLU()\n",
       "  (conv_4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (content_loss_4): ContentLoss()\n",
       "  (style_loss_4): StyleLoss()\n",
       "  (relu_4): ReLU()\n",
       "  (pool_4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (style_loss_5): StyleLoss()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img = torch.randn(1, 3, *img_size, device=device)\n",
    "model, content_losses, style_losses = get_model_and_losses(\n",
    "    content_img, style_img, default_content_layers, default_style_layers)\n",
    "\n",
    "input_img.requires_grad_(True)\n",
    "model.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d49d43",
   "metadata": {},
   "source": [
    "### 4.4. Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adc2b93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "Prev Loss: 2953.3984375\n",
      "Loss: 2953.3984375\n",
      "Step 2:\n",
      "Prev Loss: 2951.595458984375\n",
      "Loss: 2951.595458984375\n",
      "Step 3:\n",
      "Prev Loss: 702.031982421875\n",
      "Loss: 702.031982421875\n",
      "Step 4:\n",
      "Prev Loss: 387.2135009765625\n",
      "Loss: 387.2135009765625\n",
      "Step 5:\n",
      "Prev Loss: 201.2799835205078\n",
      "Loss: 201.2799835205078\n",
      "Step 6:\n",
      "Prev Loss: 123.96598815917969\n",
      "Loss: 123.96598815917969\n",
      "Step 7:\n",
      "Prev Loss: 90.75457763671875\n",
      "Loss: 90.75457763671875\n",
      "Step 8:\n",
      "Prev Loss: 73.46826171875\n",
      "Loss: 73.46826171875\n",
      "Step 9:\n",
      "Prev Loss: 62.302764892578125\n",
      "Loss: 62.302764892578125\n",
      "Step 10:\n",
      "Prev Loss: 54.568519592285156\n",
      "Loss: 54.568519592285156\n",
      "Step 11:\n",
      "Prev Loss: 49.43915939331055\n",
      "Loss: 49.43915939331055\n",
      "Step 12:\n",
      "Prev Loss: 45.83957290649414\n",
      "Loss: 45.83957290649414\n",
      "Step 13:\n",
      "Prev Loss: 42.9759521484375\n",
      "Loss: 42.9759521484375\n",
      "Step 14:\n",
      "Prev Loss: 39.94908905029297\n",
      "Loss: 39.94908905029297\n",
      "Step 15:\n",
      "Prev Loss: 37.23042297363281\n",
      "Loss: 37.23042297363281\n",
      "Step 16:\n",
      "Prev Loss: 34.981529235839844\n",
      "Loss: 34.981529235839844\n",
      "Step 17:\n",
      "Prev Loss: 33.11949920654297\n",
      "Loss: 33.11949920654297\n",
      "Step 18:\n",
      "Prev Loss: 31.629173278808594\n",
      "Loss: 31.629173278808594\n",
      "Step 19:\n",
      "Prev Loss: 30.336669921875\n",
      "Loss: 30.336669921875\n",
      "Step 20:\n",
      "Prev Loss: 29.41612434387207\n",
      "Loss: 29.41612434387207\n",
      "Step 21:\n",
      "Prev Loss: 28.23484992980957\n",
      "Loss: 28.23484992980957\n",
      "Step 22:\n",
      "Prev Loss: 27.260211944580078\n",
      "Loss: 27.260211944580078\n",
      "Step 23:\n",
      "Prev Loss: 26.43780517578125\n",
      "Loss: 26.43780517578125\n",
      "Step 24:\n",
      "Prev Loss: 25.699769973754883\n",
      "Loss: 25.699769973754883\n",
      "Step 25:\n",
      "Prev Loss: 25.155418395996094\n",
      "Loss: 25.155418395996094\n",
      "Step 26:\n",
      "Prev Loss: 24.5996150970459\n",
      "Loss: 24.5996150970459\n",
      "Step 27:\n",
      "Prev Loss: 24.049619674682617\n",
      "Loss: 24.049619674682617\n",
      "Step 28:\n",
      "Prev Loss: 23.514514923095703\n",
      "Loss: 23.514514923095703\n",
      "Step 29:\n",
      "Prev Loss: 23.142580032348633\n",
      "Loss: 23.142580032348633\n",
      "Step 30:\n",
      "Prev Loss: 22.848678588867188\n",
      "Loss: 22.848678588867188\n",
      "Step 31:\n",
      "Prev Loss: 22.394521713256836\n",
      "Loss: 22.394521713256836\n",
      "Step 32:\n",
      "Prev Loss: 22.090457916259766\n",
      "Loss: 22.090457916259766\n",
      "Step 33:\n",
      "Prev Loss: 21.770437240600586\n",
      "Loss: 21.770437240600586\n",
      "Step 34:\n",
      "Prev Loss: 21.423763275146484\n",
      "Loss: 21.423763275146484\n",
      "Step 35:\n",
      "Prev Loss: 21.10457420349121\n",
      "Loss: 21.10457420349121\n",
      "Step 36:\n",
      "Prev Loss: 20.75758171081543\n",
      "Loss: 20.75758171081543\n",
      "Step 37:\n",
      "Prev Loss: 20.444791793823242\n",
      "Loss: 20.444791793823242\n",
      "Step 38:\n",
      "Prev Loss: 20.219013214111328\n",
      "Loss: 20.219013214111328\n",
      "Step 39:\n",
      "Prev Loss: 20.004148483276367\n",
      "Loss: 20.004148483276367\n",
      "Step 40:\n",
      "Prev Loss: 19.75041961669922\n",
      "Loss: 19.75041961669922\n",
      "Step 41:\n",
      "Prev Loss: 19.47834014892578\n",
      "Loss: 19.47834014892578\n",
      "Step 42:\n",
      "Prev Loss: 19.278881072998047\n",
      "Loss: 19.278881072998047\n",
      "Step 43:\n",
      "Prev Loss: 19.097196578979492\n",
      "Loss: 19.097196578979492\n",
      "Step 44:\n",
      "Prev Loss: 18.87492561340332\n",
      "Loss: 18.87492561340332\n",
      "Step 45:\n",
      "Prev Loss: 18.730667114257812\n",
      "Loss: 18.730667114257812\n",
      "Step 46:\n",
      "Prev Loss: 18.568092346191406\n",
      "Loss: 18.568092346191406\n",
      "Step 47:\n",
      "Prev Loss: 18.424955368041992\n",
      "Loss: 18.424955368041992\n",
      "Step 48:\n",
      "Prev Loss: 18.251707077026367\n",
      "Loss: 18.251707077026367\n",
      "Step 49:\n",
      "Prev Loss: 18.057586669921875\n",
      "Loss: 18.057586669921875\n",
      "Step 50:\n",
      "Prev Loss: 17.875404357910156\n",
      "Loss: 17.875404357910156\n",
      "Step 51:\n",
      "Prev Loss: 17.739063262939453\n",
      "Loss: 17.739063262939453\n",
      "Step 52:\n",
      "Prev Loss: 17.59287452697754\n",
      "Loss: 17.59287452697754\n",
      "Step 53:\n",
      "Prev Loss: 17.447500228881836\n",
      "Loss: 17.447500228881836\n",
      "Step 54:\n",
      "Prev Loss: 17.284740447998047\n",
      "Loss: 17.284740447998047\n",
      "Step 55:\n",
      "Prev Loss: 17.130245208740234\n",
      "Loss: 17.130245208740234\n",
      "Step 56:\n",
      "Prev Loss: 16.971141815185547\n",
      "Loss: 16.971141815185547\n",
      "Step 57:\n",
      "Prev Loss: 16.84583854675293\n",
      "Loss: 16.84583854675293\n",
      "Step 58:\n",
      "Prev Loss: 16.708145141601562\n",
      "Loss: 16.708145141601562\n",
      "Step 59:\n",
      "Prev Loss: 16.649160385131836\n",
      "Loss: 16.649160385131836\n",
      "Step 60:\n",
      "Prev Loss: 16.551332473754883\n",
      "Loss: 16.551332473754883\n",
      "Step 61:\n",
      "Prev Loss: 16.465736389160156\n",
      "Loss: 16.465736389160156\n",
      "Step 62:\n",
      "Prev Loss: 16.37410545349121\n",
      "Loss: 16.37410545349121\n",
      "Step 63:\n",
      "Prev Loss: 16.2609806060791\n",
      "Loss: 16.2609806060791\n",
      "Step 64:\n",
      "Prev Loss: 16.136335372924805\n",
      "Loss: 16.136335372924805\n",
      "Step 65:\n",
      "Prev Loss: 16.04488754272461\n",
      "Loss: 16.04488754272461\n",
      "Step 66:\n",
      "Prev Loss: 15.95164680480957\n",
      "Loss: 15.95164680480957\n",
      "Step 67:\n",
      "Prev Loss: 15.860738754272461\n",
      "Loss: 15.860738754272461\n",
      "Step 68:\n",
      "Prev Loss: 15.753862380981445\n",
      "Loss: 15.753862380981445\n",
      "Step 69:\n",
      "Prev Loss: 15.655506134033203\n",
      "Loss: 15.655506134033203\n",
      "Step 70:\n",
      "Prev Loss: 15.563447952270508\n",
      "Loss: 15.563447952270508\n",
      "Step 71:\n",
      "Prev Loss: 15.476846694946289\n",
      "Loss: 15.476846694946289\n",
      "Step 72:\n",
      "Prev Loss: 15.393708229064941\n",
      "Loss: 15.393708229064941\n",
      "Step 73:\n",
      "Prev Loss: 15.337512969970703\n",
      "Loss: 15.337512969970703\n",
      "Step 74:\n",
      "Prev Loss: 15.264365196228027\n",
      "Loss: 15.264365196228027\n",
      "Step 75:\n",
      "Prev Loss: 15.200098991394043\n",
      "Loss: 15.200098991394043\n",
      "Step 76:\n",
      "Prev Loss: 15.121772766113281\n",
      "Loss: 15.121772766113281\n",
      "Step 77:\n",
      "Prev Loss: 15.042682647705078\n",
      "Loss: 15.042682647705078\n",
      "Step 78:\n",
      "Prev Loss: 14.962850570678711\n",
      "Loss: 14.962850570678711\n",
      "Step 79:\n",
      "Prev Loss: 14.88199520111084\n",
      "Loss: 14.88199520111084\n",
      "Step 80:\n",
      "Prev Loss: 14.80141544342041\n",
      "Loss: 14.80141544342041\n",
      "Step 81:\n",
      "Prev Loss: 14.728432655334473\n",
      "Loss: 14.728432655334473\n",
      "Step 82:\n",
      "Prev Loss: 14.655914306640625\n",
      "Loss: 14.655914306640625\n",
      "Step 83:\n",
      "Prev Loss: 14.589223861694336\n",
      "Loss: 14.589223861694336\n",
      "Step 84:\n",
      "Prev Loss: 14.553022384643555\n",
      "Loss: 14.553022384643555\n",
      "Step 85:\n",
      "Prev Loss: 14.496448516845703\n",
      "Loss: 14.496448516845703\n",
      "Step 86:\n",
      "Prev Loss: 14.449185371398926\n",
      "Loss: 14.449185371398926\n",
      "Step 87:\n",
      "Prev Loss: 14.397483825683594\n",
      "Loss: 14.397483825683594\n",
      "Step 88:\n",
      "Prev Loss: 14.340956687927246\n",
      "Loss: 14.340956687927246\n",
      "Step 89:\n",
      "Prev Loss: 14.279659271240234\n",
      "Loss: 14.279659271240234\n",
      "Step 90:\n",
      "Prev Loss: 14.223895072937012\n",
      "Loss: 14.223895072937012\n",
      "Step 91:\n",
      "Prev Loss: 14.166934967041016\n",
      "Loss: 14.166934967041016\n",
      "Step 92:\n",
      "Prev Loss: 14.113396644592285\n",
      "Loss: 14.113396644592285\n",
      "Step 93:\n",
      "Prev Loss: 14.05938720703125\n",
      "Loss: 14.05938720703125\n",
      "Step 94:\n",
      "Prev Loss: 14.015279769897461\n",
      "Loss: 14.015279769897461\n",
      "Step 95:\n",
      "Prev Loss: 13.970109939575195\n",
      "Loss: 13.970109939575195\n",
      "Step 96:\n",
      "Prev Loss: 13.92949104309082\n",
      "Loss: 13.92949104309082\n",
      "Step 97:\n",
      "Prev Loss: 13.874917984008789\n",
      "Loss: 13.874917984008789\n",
      "Step 98:\n",
      "Prev Loss: 13.827079772949219\n",
      "Loss: 13.827079772949219\n",
      "Step 99:\n",
      "Prev Loss: 13.780667304992676\n",
      "Loss: 13.780667304992676\n",
      "Step 100:\n",
      "Prev Loss: 13.732070922851562\n",
      "Loss: 13.732070922851562\n",
      "Step 101:\n",
      "Prev Loss: 13.685592651367188\n",
      "Loss: 13.685592651367188\n",
      "Step 102:\n",
      "Prev Loss: 13.63951587677002\n",
      "Loss: 13.63951587677002\n",
      "Step 103:\n",
      "Prev Loss: 13.59168529510498\n",
      "Loss: 13.59168529510498\n",
      "Step 104:\n",
      "Prev Loss: 13.557912826538086\n",
      "Loss: 13.557912826538086\n",
      "Step 105:\n",
      "Prev Loss: 13.515765190124512\n",
      "Loss: 13.515765190124512\n",
      "Step 106:\n",
      "Prev Loss: 13.48119068145752\n",
      "Loss: 13.48119068145752\n",
      "Step 107:\n",
      "Prev Loss: 13.43957805633545\n",
      "Loss: 13.43957805633545\n",
      "Step 108:\n",
      "Prev Loss: 13.3990478515625\n",
      "Loss: 13.3990478515625\n",
      "Step 109:\n",
      "Prev Loss: 13.363798141479492\n",
      "Loss: 13.363798141479492\n",
      "Step 110:\n",
      "Prev Loss: 13.327400207519531\n",
      "Loss: 13.327400207519531\n",
      "Step 111:\n",
      "Prev Loss: 13.28881549835205\n",
      "Loss: 13.28881549835205\n",
      "Step 112:\n",
      "Prev Loss: 13.249103546142578\n",
      "Loss: 13.249103546142578\n",
      "Step 113:\n",
      "Prev Loss: 13.207574844360352\n",
      "Loss: 13.207574844360352\n",
      "Step 114:\n",
      "Prev Loss: 13.171272277832031\n",
      "Loss: 13.171272277832031\n",
      "Step 115:\n",
      "Prev Loss: 13.13598918914795\n",
      "Loss: 13.13598918914795\n",
      "Step 116:\n",
      "Prev Loss: 13.102210998535156\n",
      "Loss: 13.102210998535156\n",
      "Step 117:\n",
      "Prev Loss: 13.065207481384277\n",
      "Loss: 13.065207481384277\n",
      "Step 118:\n",
      "Prev Loss: 13.035811424255371\n",
      "Loss: 13.035811424255371\n",
      "Step 119:\n",
      "Prev Loss: 13.005690574645996\n",
      "Loss: 13.005690574645996\n",
      "Step 120:\n",
      "Prev Loss: 12.984130859375\n",
      "Loss: 12.984130859375\n",
      "Step 121:\n",
      "Prev Loss: 12.954621315002441\n",
      "Loss: 12.954621315002441\n",
      "Step 122:\n",
      "Prev Loss: 12.92244815826416\n",
      "Loss: 12.92244815826416\n",
      "Step 123:\n",
      "Prev Loss: 12.892659187316895\n",
      "Loss: 12.892659187316895\n",
      "Step 124:\n",
      "Prev Loss: 12.865340232849121\n",
      "Loss: 12.865340232849121\n",
      "Step 125:\n",
      "Prev Loss: 12.83747673034668\n",
      "Loss: 12.83747673034668\n",
      "Step 126:\n",
      "Prev Loss: 12.810705184936523\n",
      "Loss: 12.810705184936523\n",
      "Step 127:\n",
      "Prev Loss: 12.785290718078613\n",
      "Loss: 12.785290718078613\n",
      "Step 128:\n",
      "Prev Loss: 12.759807586669922\n",
      "Loss: 12.759807586669922\n",
      "Step 129:\n",
      "Prev Loss: 12.733903884887695\n",
      "Loss: 12.733903884887695\n",
      "Step 130:\n",
      "Prev Loss: 12.708070755004883\n",
      "Loss: 12.708070755004883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 131:\n",
      "Prev Loss: 12.684412002563477\n",
      "Loss: 12.684412002563477\n",
      "Step 132:\n",
      "Prev Loss: 12.660452842712402\n",
      "Loss: 12.660452842712402\n",
      "Step 133:\n",
      "Prev Loss: 12.637665748596191\n",
      "Loss: 12.637665748596191\n",
      "Step 134:\n",
      "Prev Loss: 12.615509033203125\n",
      "Loss: 12.615509033203125\n",
      "Step 135:\n",
      "Prev Loss: 12.601007461547852\n",
      "Loss: 12.601007461547852\n",
      "Step 136:\n",
      "Prev Loss: 12.57907772064209\n",
      "Loss: 12.57907772064209\n",
      "Step 137:\n",
      "Prev Loss: 12.562492370605469\n",
      "Loss: 12.562492370605469\n",
      "Step 138:\n",
      "Prev Loss: 12.53976821899414\n",
      "Loss: 12.53976821899414\n",
      "Step 139:\n",
      "Prev Loss: 12.51814079284668\n",
      "Loss: 12.51814079284668\n",
      "Step 140:\n",
      "Prev Loss: 12.49918270111084\n",
      "Loss: 12.49918270111084\n",
      "Step 141:\n",
      "Prev Loss: 12.481354713439941\n",
      "Loss: 12.481354713439941\n",
      "Step 142:\n",
      "Prev Loss: 12.463284492492676\n",
      "Loss: 12.463284492492676\n",
      "Step 143:\n",
      "Prev Loss: 12.443449974060059\n",
      "Loss: 12.443449974060059\n",
      "Step 144:\n",
      "Prev Loss: 12.423367500305176\n",
      "Loss: 12.423367500305176\n",
      "Step 145:\n",
      "Prev Loss: 12.404609680175781\n",
      "Loss: 12.404609680175781\n",
      "Step 146:\n",
      "Prev Loss: 12.385761260986328\n",
      "Loss: 12.385761260986328\n",
      "Step 147:\n",
      "Prev Loss: 12.367650985717773\n",
      "Loss: 12.367650985717773\n",
      "Step 148:\n",
      "Prev Loss: 12.348464965820312\n",
      "Loss: 12.348464965820312\n",
      "Step 149:\n",
      "Prev Loss: 12.33197021484375\n",
      "Loss: 12.33197021484375\n",
      "Step 150:\n",
      "Prev Loss: 12.311904907226562\n",
      "Loss: 12.311904907226562\n",
      "Step 151:\n",
      "Prev Loss: 12.30090618133545\n",
      "Loss: 12.30090618133545\n",
      "Step 152:\n",
      "Prev Loss: 12.280500411987305\n",
      "Loss: 12.280500411987305\n",
      "Step 153:\n",
      "Prev Loss: 12.264729499816895\n",
      "Loss: 12.264729499816895\n",
      "Step 154:\n",
      "Prev Loss: 12.24791145324707\n",
      "Loss: 12.24791145324707\n",
      "Step 155:\n",
      "Prev Loss: 12.232430458068848\n",
      "Loss: 12.232430458068848\n",
      "Step 156:\n",
      "Prev Loss: 12.21746826171875\n",
      "Loss: 12.21746826171875\n",
      "Step 157:\n",
      "Prev Loss: 12.201148986816406\n",
      "Loss: 12.201148986816406\n",
      "Step 158:\n",
      "Prev Loss: 12.18764591217041\n",
      "Loss: 12.18764591217041\n",
      "Step 159:\n",
      "Prev Loss: 12.17147445678711\n",
      "Loss: 12.17147445678711\n",
      "Step 160:\n",
      "Prev Loss: 12.159934043884277\n",
      "Loss: 12.159934043884277\n",
      "Step 161:\n",
      "Prev Loss: 12.142831802368164\n",
      "Loss: 12.142831802368164\n",
      "Step 162:\n",
      "Prev Loss: 12.129645347595215\n",
      "Loss: 12.129645347595215\n",
      "Step 163:\n",
      "Prev Loss: 12.113792419433594\n",
      "Loss: 12.113792419433594\n",
      "Step 164:\n",
      "Prev Loss: 12.099291801452637\n",
      "Loss: 12.099291801452637\n",
      "Step 165:\n",
      "Prev Loss: 12.088857650756836\n",
      "Loss: 12.088857650756836\n",
      "Step 166:\n",
      "Prev Loss: 12.073819160461426\n",
      "Loss: 12.073819160461426\n",
      "Step 167:\n",
      "Prev Loss: 12.05888557434082\n",
      "Loss: 12.05888557434082\n",
      "Step 168:\n",
      "Prev Loss: 12.050559997558594\n",
      "Loss: 12.050559997558594\n",
      "Step 169:\n",
      "Prev Loss: 12.036434173583984\n",
      "Loss: 12.036434173583984\n",
      "Step 170:\n",
      "Prev Loss: 12.028312683105469\n",
      "Loss: 12.028312683105469\n",
      "Step 171:\n",
      "Prev Loss: 12.010616302490234\n",
      "Loss: 12.010616302490234\n",
      "Step 172:\n",
      "Prev Loss: 12.001142501831055\n",
      "Loss: 12.001142501831055\n",
      "Step 173:\n",
      "Prev Loss: 11.992288589477539\n",
      "Loss: 11.992288589477539\n",
      "Step 174:\n",
      "Prev Loss: 11.977027893066406\n",
      "Loss: 11.977027893066406\n",
      "Step 175:\n",
      "Prev Loss: 11.967377662658691\n",
      "Loss: 11.967377662658691\n",
      "Step 176:\n",
      "Prev Loss: 11.953303337097168\n",
      "Loss: 11.953303337097168\n",
      "Step 177:\n",
      "Prev Loss: 11.945100784301758\n",
      "Loss: 11.945100784301758\n",
      "Step 178:\n",
      "Prev Loss: 11.932635307312012\n",
      "Loss: 11.932635307312012\n",
      "Step 179:\n",
      "Prev Loss: 11.919439315795898\n",
      "Loss: 11.919439315795898\n",
      "Step 180:\n",
      "Prev Loss: 11.911867141723633\n",
      "Loss: 11.911867141723633\n",
      "Step 181:\n",
      "Prev Loss: 11.901063919067383\n",
      "Loss: 11.901063919067383\n",
      "Step 182:\n",
      "Prev Loss: 11.892380714416504\n",
      "Loss: 11.892380714416504\n",
      "Step 183:\n",
      "Prev Loss: 11.880826950073242\n",
      "Loss: 11.880826950073242\n",
      "Step 184:\n",
      "Prev Loss: 11.870923042297363\n",
      "Loss: 11.870923042297363\n",
      "Step 185:\n",
      "Prev Loss: 11.861188888549805\n",
      "Loss: 11.861188888549805\n",
      "Step 186:\n",
      "Prev Loss: 11.850887298583984\n",
      "Loss: 11.850887298583984\n",
      "Step 187:\n",
      "Prev Loss: 11.846291542053223\n",
      "Loss: 11.846291542053223\n",
      "Step 188:\n",
      "Prev Loss: 11.833724975585938\n",
      "Loss: 11.833724975585938\n",
      "Step 189:\n",
      "Prev Loss: 11.826653480529785\n",
      "Loss: 11.826653480529785\n",
      "Step 190:\n",
      "Prev Loss: 11.81412410736084\n",
      "Loss: 11.81412410736084\n",
      "Step 191:\n",
      "Prev Loss: 11.805450439453125\n",
      "Loss: 11.805450439453125\n",
      "Step 192:\n",
      "Prev Loss: 11.800018310546875\n",
      "Loss: 11.800018310546875\n",
      "Step 193:\n",
      "Prev Loss: 11.789063453674316\n",
      "Loss: 11.789063453674316\n",
      "Step 194:\n",
      "Prev Loss: 11.782950401306152\n",
      "Loss: 11.782950401306152\n",
      "Step 195:\n",
      "Prev Loss: 11.773935317993164\n",
      "Loss: 11.773935317993164\n",
      "Step 196:\n",
      "Prev Loss: 11.764142036437988\n",
      "Loss: 11.764142036437988\n",
      "Step 197:\n",
      "Prev Loss: 11.761686325073242\n",
      "Loss: 11.761686325073242\n",
      "Step 198:\n",
      "Prev Loss: 11.749038696289062\n",
      "Loss: 11.749038696289062\n",
      "Step 199:\n",
      "Prev Loss: 11.752614974975586\n",
      "Loss: 11.752614974975586\n",
      "Step 200:\n",
      "Prev Loss: 11.736687660217285\n",
      "Loss: 11.736687660217285\n",
      "Step 201:\n",
      "Prev Loss: 11.722740173339844\n",
      "Loss: 11.722740173339844\n",
      "Step 202:\n",
      "Prev Loss: 11.728764533996582\n",
      "Loss: 11.728764533996582\n",
      "Step 203:\n",
      "Prev Loss: 11.713622093200684\n",
      "Loss: 11.713622093200684\n",
      "Step 204:\n",
      "Prev Loss: 11.706378936767578\n",
      "Loss: 11.706378936767578\n",
      "Step 205:\n",
      "Prev Loss: 11.697640419006348\n",
      "Loss: 11.697640419006348\n",
      "Step 206:\n",
      "Prev Loss: 11.687296867370605\n",
      "Loss: 11.687296867370605\n",
      "Step 207:\n",
      "Prev Loss: 11.685220718383789\n",
      "Loss: 11.685220718383789\n",
      "Step 208:\n",
      "Prev Loss: 11.685997009277344\n",
      "Loss: 11.685997009277344\n",
      "Step 209:\n",
      "Prev Loss: 11.676005363464355\n",
      "Loss: 11.676005363464355\n",
      "Step 210:\n",
      "Prev Loss: 11.661670684814453\n",
      "Loss: 11.661670684814453\n",
      "Step 211:\n",
      "Prev Loss: 11.658493041992188\n",
      "Loss: 11.658493041992188\n",
      "Step 212:\n",
      "Prev Loss: 11.655973434448242\n",
      "Loss: 11.655973434448242\n",
      "Step 213:\n",
      "Prev Loss: 11.657939910888672\n",
      "Loss: 11.657939910888672\n",
      "Step 214:\n",
      "Prev Loss: 11.65079116821289\n",
      "Loss: 11.65079116821289\n",
      "Step 215:\n",
      "Prev Loss: 11.649137496948242\n",
      "Loss: 11.649137496948242\n",
      "Step 216:\n",
      "Prev Loss: 11.639841079711914\n",
      "Loss: 11.639841079711914\n",
      "Step 217:\n",
      "Prev Loss: 11.644001960754395\n",
      "Loss: 11.644001960754395\n",
      "Step 218:\n",
      "Prev Loss: 11.641118049621582\n",
      "Loss: 11.641118049621582\n",
      "Step 219:\n",
      "Prev Loss: 11.636432647705078\n",
      "Loss: 11.636432647705078\n",
      "Step 220:\n",
      "Prev Loss: 11.632329940795898\n",
      "Loss: 11.632329940795898\n",
      "Step 221:\n",
      "Prev Loss: 11.621585845947266\n",
      "Loss: 11.621585845947266\n",
      "Step 222:\n",
      "Prev Loss: 11.619067192077637\n",
      "Loss: 11.619067192077637\n",
      "Step 223:\n",
      "Prev Loss: 11.615851402282715\n",
      "Loss: 11.615851402282715\n",
      "Step 224:\n",
      "Prev Loss: 11.623848915100098\n",
      "Loss: 11.623848915100098\n",
      "Step 225:\n",
      "Prev Loss: 11.617311477661133\n",
      "Loss: 11.617311477661133\n",
      "Step 226:\n",
      "Prev Loss: 11.618936538696289\n",
      "Loss: 11.618936538696289\n",
      "Step 227:\n",
      "Prev Loss: 11.615135192871094\n",
      "Loss: 11.615135192871094\n",
      "Step 228:\n",
      "Prev Loss: 11.646804809570312\n",
      "Loss: 11.646804809570312\n",
      "Step 229:\n",
      "Prev Loss: 11.643651962280273\n",
      "Loss: 11.643651962280273\n",
      "Step 230:\n",
      "Prev Loss: 11.613709449768066\n",
      "Loss: 11.613709449768066\n",
      "Step 231:\n",
      "Prev Loss: 11.726202011108398\n",
      "Loss: 11.726202011108398\n",
      "Step 232:\n",
      "Prev Loss: 11.752192497253418\n",
      "Loss: 11.752192497253418\n",
      "Step 233:\n",
      "Prev Loss: 12.877015113830566\n",
      "Loss: 12.877015113830566\n",
      "Step 234:\n",
      "Prev Loss: 11.998748779296875\n",
      "Loss: 11.998748779296875\n",
      "Step 235:\n",
      "Prev Loss: 11.826730728149414\n",
      "Loss: 11.826730728149414\n",
      "Step 236:\n",
      "Prev Loss: 11.861794471740723\n",
      "Loss: 11.861794471740723\n",
      "Step 237:\n",
      "Prev Loss: 11.739344596862793\n",
      "Loss: 11.739344596862793\n",
      "Step 238:\n",
      "Prev Loss: 11.714184761047363\n",
      "Loss: 11.714184761047363\n",
      "Step 239:\n",
      "Prev Loss: 11.68526840209961\n",
      "Loss: 11.68526840209961\n",
      "Step 240:\n",
      "Prev Loss: 11.667531967163086\n",
      "Loss: 11.667531967163086\n",
      "Step 241:\n",
      "Prev Loss: 11.634859085083008\n",
      "Loss: 11.634859085083008\n",
      "Step 242:\n",
      "Prev Loss: 11.619573593139648\n",
      "Loss: 11.619573593139648\n",
      "Step 243:\n",
      "Prev Loss: 11.605836868286133\n",
      "Loss: 11.605836868286133\n",
      "Step 244:\n",
      "Prev Loss: 11.589128494262695\n",
      "Loss: 11.589128494262695\n",
      "Step 245:\n",
      "Prev Loss: 11.574583053588867\n",
      "Loss: 11.574583053588867\n",
      "Step 246:\n",
      "Prev Loss: 11.568273544311523\n",
      "Loss: 11.568273544311523\n",
      "Step 247:\n",
      "Prev Loss: 11.569602966308594\n",
      "Loss: 11.569602966308594\n",
      "Step 248:\n",
      "Prev Loss: 11.579890251159668\n",
      "Loss: 11.579890251159668\n",
      "Step 249:\n",
      "Prev Loss: 11.612476348876953\n",
      "Loss: 11.612476348876953\n",
      "Step 250:\n",
      "Prev Loss: 11.675727844238281\n",
      "Loss: 11.675727844238281\n",
      "Step 251:\n",
      "Prev Loss: 11.746906280517578\n",
      "Loss: 11.746906280517578\n",
      "Step 252:\n",
      "Prev Loss: 11.790098190307617\n",
      "Loss: 11.790098190307617\n",
      "Step 253:\n",
      "Prev Loss: 11.777864456176758\n",
      "Loss: 11.777864456176758\n",
      "Step 254:\n",
      "Prev Loss: 11.790375709533691\n",
      "Loss: 11.790375709533691\n",
      "Step 255:\n",
      "Prev Loss: 11.886996269226074\n",
      "Loss: 11.886996269226074\n",
      "Step 256:\n",
      "Prev Loss: 12.002538681030273\n",
      "Loss: 12.002538681030273\n",
      "Step 257:\n",
      "Prev Loss: 11.949674606323242\n",
      "Loss: 11.949674606323242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 258:\n",
      "Prev Loss: 12.488736152648926\n",
      "Loss: 12.488736152648926\n",
      "Step 259:\n",
      "Prev Loss: 11.914255142211914\n",
      "Loss: 11.914255142211914\n",
      "Step 260:\n",
      "Prev Loss: 11.904149055480957\n",
      "Loss: 11.904149055480957\n",
      "Step 261:\n",
      "Prev Loss: 11.839544296264648\n",
      "Loss: 11.839544296264648\n",
      "Step 262:\n",
      "Prev Loss: 11.808177947998047\n",
      "Loss: 11.808177947998047\n",
      "Step 263:\n",
      "Prev Loss: 11.786237716674805\n",
      "Loss: 11.786237716674805\n",
      "Step 264:\n",
      "Prev Loss: 11.747186660766602\n",
      "Loss: 11.747186660766602\n",
      "Step 265:\n",
      "Prev Loss: 11.719892501831055\n",
      "Loss: 11.719892501831055\n",
      "Step 266:\n",
      "Prev Loss: 11.694402694702148\n",
      "Loss: 11.694402694702148\n",
      "Step 267:\n",
      "Prev Loss: 11.671867370605469\n",
      "Loss: 11.671867370605469\n",
      "Step 268:\n",
      "Prev Loss: 11.65311050415039\n",
      "Loss: 11.65311050415039\n",
      "Step 269:\n",
      "Prev Loss: 11.642206192016602\n",
      "Loss: 11.642206192016602\n",
      "Step 270:\n",
      "Prev Loss: 11.631991386413574\n",
      "Loss: 11.631991386413574\n",
      "Step 271:\n",
      "Prev Loss: 11.62299919128418\n",
      "Loss: 11.62299919128418\n",
      "Step 272:\n",
      "Prev Loss: 11.62265396118164\n",
      "Loss: 11.62265396118164\n",
      "Step 273:\n",
      "Prev Loss: 11.632696151733398\n",
      "Loss: 11.632696151733398\n",
      "Step 274:\n",
      "Prev Loss: 11.653667449951172\n",
      "Loss: 11.653667449951172\n",
      "Step 275:\n",
      "Prev Loss: 11.700162887573242\n",
      "Loss: 11.700162887573242\n",
      "Step 276:\n",
      "Prev Loss: 11.766315460205078\n",
      "Loss: 11.766315460205078\n",
      "Step 277:\n",
      "Prev Loss: 11.783666610717773\n",
      "Loss: 11.783666610717773\n",
      "Step 278:\n",
      "Prev Loss: 11.767266273498535\n",
      "Loss: 11.767266273498535\n",
      "Step 279:\n",
      "Prev Loss: 11.784408569335938\n",
      "Loss: 11.784408569335938\n",
      "Step 280:\n",
      "Prev Loss: 12.06021499633789\n",
      "Loss: 12.06021499633789\n",
      "Step 281:\n",
      "Prev Loss: 12.61127758026123\n",
      "Loss: 12.61127758026123\n",
      "Step 282:\n",
      "Prev Loss: 12.553011894226074\n",
      "Loss: 12.553011894226074\n",
      "Step 283:\n",
      "Prev Loss: 13.930066108703613\n",
      "Loss: 13.930066108703613\n",
      "Step 284:\n",
      "Prev Loss: 18.42532730102539\n",
      "Loss: 18.42532730102539\n",
      "Step 285:\n",
      "Prev Loss: 13.486425399780273\n",
      "Loss: 13.486425399780273\n",
      "Step 286:\n",
      "Prev Loss: 13.231388092041016\n",
      "Loss: 13.231388092041016\n",
      "Step 287:\n",
      "Prev Loss: 12.84598159790039\n",
      "Loss: 12.84598159790039\n",
      "Step 288:\n",
      "Prev Loss: 12.782194137573242\n",
      "Loss: 12.782194137573242\n",
      "Step 289:\n",
      "Prev Loss: 12.524720191955566\n",
      "Loss: 12.524720191955566\n",
      "Step 290:\n",
      "Prev Loss: 12.48258113861084\n",
      "Loss: 12.48258113861084\n",
      "Step 291:\n",
      "Prev Loss: 12.361688613891602\n",
      "Loss: 12.361688613891602\n",
      "Step 292:\n",
      "Prev Loss: 12.26560115814209\n",
      "Loss: 12.26560115814209\n",
      "Step 293:\n",
      "Prev Loss: 12.185234069824219\n",
      "Loss: 12.185234069824219\n",
      "Step 294:\n",
      "Prev Loss: 12.141181945800781\n",
      "Loss: 12.141181945800781\n",
      "Step 295:\n",
      "Prev Loss: 12.084627151489258\n",
      "Loss: 12.084627151489258\n",
      "Step 296:\n",
      "Prev Loss: 12.037491798400879\n",
      "Loss: 12.037491798400879\n",
      "Step 297:\n",
      "Prev Loss: 11.99536418914795\n",
      "Loss: 11.99536418914795\n",
      "Step 298:\n",
      "Prev Loss: 11.957412719726562\n",
      "Loss: 11.957412719726562\n",
      "Step 299:\n",
      "Prev Loss: 11.925533294677734\n",
      "Loss: 11.925533294677734\n",
      "Step 300:\n",
      "Prev Loss: 11.910638809204102\n",
      "Loss: 11.910638809204102\n",
      "Step 301:\n",
      "Prev Loss: 11.879196166992188\n",
      "Loss: 11.879196166992188\n",
      "Step 302:\n",
      "Prev Loss: 11.858626365661621\n",
      "Loss: 11.858626365661621\n",
      "Step 303:\n",
      "Prev Loss: 11.84592342376709\n",
      "Loss: 11.84592342376709\n",
      "Step 304:\n",
      "Prev Loss: 11.834674835205078\n",
      "Loss: 11.834674835205078\n",
      "Step 305:\n",
      "Prev Loss: 11.818846702575684\n",
      "Loss: 11.818846702575684\n",
      "Step 306:\n",
      "Prev Loss: 11.806981086730957\n",
      "Loss: 11.806981086730957\n",
      "Step 307:\n",
      "Prev Loss: 11.7955322265625\n",
      "Loss: 11.7955322265625\n",
      "Step 308:\n",
      "Prev Loss: 11.787833213806152\n",
      "Loss: 11.787833213806152\n",
      "Step 309:\n",
      "Prev Loss: 11.784204483032227\n",
      "Loss: 11.784204483032227\n",
      "Step 310:\n",
      "Prev Loss: 11.806166648864746\n",
      "Loss: 11.806166648864746\n",
      "Step 311:\n",
      "Prev Loss: 11.855365753173828\n",
      "Loss: 11.855365753173828\n",
      "Step 312:\n",
      "Prev Loss: 11.893731117248535\n",
      "Loss: 11.893731117248535\n",
      "Step 313:\n",
      "Prev Loss: 11.887678146362305\n",
      "Loss: 11.887678146362305\n",
      "Step 314:\n",
      "Prev Loss: 11.893621444702148\n",
      "Loss: 11.893621444702148\n",
      "Step 315:\n",
      "Prev Loss: 11.894536972045898\n",
      "Loss: 11.894536972045898\n",
      "Step 316:\n",
      "Prev Loss: 11.949308395385742\n",
      "Loss: 11.949308395385742\n",
      "Step 317:\n",
      "Prev Loss: 11.966761589050293\n",
      "Loss: 11.966761589050293\n",
      "Step 318:\n",
      "Prev Loss: 11.967564582824707\n",
      "Loss: 11.967564582824707\n",
      "Step 319:\n",
      "Prev Loss: 12.01557445526123\n",
      "Loss: 12.01557445526123\n",
      "Step 320:\n",
      "Prev Loss: 14.175025939941406\n",
      "Loss: 14.175025939941406\n",
      "Step 321:\n",
      "Prev Loss: 20.72374725341797\n",
      "Loss: 20.72374725341797\n",
      "Step 322:\n",
      "Prev Loss: 14.462095260620117\n",
      "Loss: 14.462095260620117\n",
      "Step 323:\n",
      "Prev Loss: 13.691084861755371\n",
      "Loss: 13.691084861755371\n",
      "Step 324:\n",
      "Prev Loss: 12.884038925170898\n",
      "Loss: 12.884038925170898\n",
      "Step 325:\n",
      "Prev Loss: 12.815842628479004\n",
      "Loss: 12.815842628479004\n",
      "Step 326:\n",
      "Prev Loss: 12.60032844543457\n",
      "Loss: 12.60032844543457\n",
      "Step 327:\n",
      "Prev Loss: 12.50882339477539\n",
      "Loss: 12.50882339477539\n",
      "Step 328:\n",
      "Prev Loss: 12.43467903137207\n",
      "Loss: 12.43467903137207\n",
      "Step 329:\n",
      "Prev Loss: 12.356269836425781\n",
      "Loss: 12.356269836425781\n",
      "Step 330:\n",
      "Prev Loss: 12.28012466430664\n",
      "Loss: 12.28012466430664\n",
      "Step 331:\n",
      "Prev Loss: 12.219639778137207\n",
      "Loss: 12.219639778137207\n",
      "Step 332:\n",
      "Prev Loss: 12.168614387512207\n",
      "Loss: 12.168614387512207\n",
      "Step 333:\n",
      "Prev Loss: 12.129365921020508\n",
      "Loss: 12.129365921020508\n",
      "Step 334:\n",
      "Prev Loss: 12.097387313842773\n",
      "Loss: 12.097387313842773\n",
      "Step 335:\n",
      "Prev Loss: 12.074714660644531\n",
      "Loss: 12.074714660644531\n",
      "Step 336:\n",
      "Prev Loss: 12.056073188781738\n",
      "Loss: 12.056073188781738\n",
      "Step 337:\n",
      "Prev Loss: 12.036518096923828\n",
      "Loss: 12.036518096923828\n",
      "Step 338:\n",
      "Prev Loss: 12.048163414001465\n",
      "Loss: 12.048163414001465\n",
      "Step 339:\n",
      "Prev Loss: 12.050676345825195\n",
      "Loss: 12.050676345825195\n",
      "Step 340:\n",
      "Prev Loss: 12.038630485534668\n",
      "Loss: 12.038630485534668\n",
      "Step 341:\n",
      "Prev Loss: 12.035446166992188\n",
      "Loss: 12.035446166992188\n",
      "Step 342:\n",
      "Prev Loss: 12.041997909545898\n",
      "Loss: 12.041997909545898\n",
      "Step 343:\n",
      "Prev Loss: 12.071496963500977\n",
      "Loss: 12.071496963500977\n",
      "Step 344:\n",
      "Prev Loss: 12.10936450958252\n",
      "Loss: 12.10936450958252\n",
      "Step 345:\n",
      "Prev Loss: 12.128554344177246\n",
      "Loss: 12.128554344177246\n",
      "Step 346:\n",
      "Prev Loss: 12.130362510681152\n",
      "Loss: 12.130362510681152\n",
      "Step 347:\n",
      "Prev Loss: 12.324922561645508\n",
      "Loss: 12.324922561645508\n",
      "Step 348:\n",
      "Prev Loss: 12.573760032653809\n",
      "Loss: 12.573760032653809\n",
      "Step 349:\n",
      "Prev Loss: 12.507568359375\n",
      "Loss: 12.507568359375\n",
      "Step 350:\n",
      "Prev Loss: 13.675994873046875\n",
      "Loss: 13.675994873046875\n",
      "Step 351:\n",
      "Prev Loss: 14.958712577819824\n",
      "Loss: 14.958712577819824\n",
      "Step 352:\n",
      "Prev Loss: 12.662331581115723\n",
      "Loss: 12.662331581115723\n",
      "Step 353:\n",
      "Prev Loss: 12.464862823486328\n",
      "Loss: 12.464862823486328\n",
      "Step 354:\n",
      "Prev Loss: 12.437332153320312\n",
      "Loss: 12.437332153320312\n",
      "Step 355:\n",
      "Prev Loss: 12.33701229095459\n",
      "Loss: 12.33701229095459\n",
      "Step 356:\n",
      "Prev Loss: 12.298151016235352\n",
      "Loss: 12.298151016235352\n",
      "Step 357:\n",
      "Prev Loss: 12.271397590637207\n",
      "Loss: 12.271397590637207\n",
      "Step 358:\n",
      "Prev Loss: 12.231237411499023\n",
      "Loss: 12.231237411499023\n",
      "Step 359:\n",
      "Prev Loss: 12.19202995300293\n",
      "Loss: 12.19202995300293\n",
      "Step 360:\n",
      "Prev Loss: 12.172273635864258\n",
      "Loss: 12.172273635864258\n",
      "Step 361:\n",
      "Prev Loss: 12.193023681640625\n",
      "Loss: 12.193023681640625\n",
      "Step 362:\n",
      "Prev Loss: 12.225378036499023\n",
      "Loss: 12.225378036499023\n",
      "Step 363:\n",
      "Prev Loss: 12.269400596618652\n",
      "Loss: 12.269400596618652\n",
      "Step 364:\n",
      "Prev Loss: 12.322213172912598\n",
      "Loss: 12.322213172912598\n",
      "Step 365:\n",
      "Prev Loss: 12.432358741760254\n",
      "Loss: 12.432358741760254\n",
      "Step 366:\n",
      "Prev Loss: 12.933929443359375\n",
      "Loss: 12.933929443359375\n",
      "Step 367:\n",
      "Prev Loss: 13.396064758300781\n",
      "Loss: 13.396064758300781\n",
      "Step 368:\n",
      "Prev Loss: 13.216647148132324\n",
      "Loss: 13.216647148132324\n",
      "Step 369:\n",
      "Prev Loss: 18.144283294677734\n",
      "Loss: 18.144283294677734\n",
      "Step 370:\n",
      "Prev Loss: 13.028223037719727\n",
      "Loss: 13.028223037719727\n",
      "Step 371:\n",
      "Prev Loss: 12.969535827636719\n",
      "Loss: 12.969535827636719\n",
      "Step 372:\n",
      "Prev Loss: 12.832487106323242\n",
      "Loss: 12.832487106323242\n",
      "Step 373:\n",
      "Prev Loss: 12.791571617126465\n",
      "Loss: 12.791571617126465\n",
      "Step 374:\n",
      "Prev Loss: 12.73543643951416\n",
      "Loss: 12.73543643951416\n",
      "Step 375:\n",
      "Prev Loss: 12.648015022277832\n",
      "Loss: 12.648015022277832\n",
      "Step 376:\n",
      "Prev Loss: 12.570938110351562\n",
      "Loss: 12.570938110351562\n",
      "Step 377:\n",
      "Prev Loss: 12.517582893371582\n",
      "Loss: 12.517582893371582\n",
      "Step 378:\n",
      "Prev Loss: 12.474581718444824\n",
      "Loss: 12.474581718444824\n",
      "Step 379:\n",
      "Prev Loss: 12.441591262817383\n",
      "Loss: 12.441591262817383\n",
      "Step 380:\n",
      "Prev Loss: 12.400293350219727\n",
      "Loss: 12.400293350219727\n",
      "Step 381:\n",
      "Prev Loss: 12.370551109313965\n",
      "Loss: 12.370551109313965\n",
      "Step 382:\n",
      "Prev Loss: 12.373285293579102\n",
      "Loss: 12.373285293579102\n",
      "Step 383:\n",
      "Prev Loss: 12.348154067993164\n",
      "Loss: 12.348154067993164\n",
      "Step 384:\n",
      "Prev Loss: 12.321879386901855\n",
      "Loss: 12.321879386901855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 385:\n",
      "Prev Loss: 12.319891929626465\n",
      "Loss: 12.319891929626465\n",
      "Step 386:\n",
      "Prev Loss: 12.381745338439941\n",
      "Loss: 12.381745338439941\n",
      "Step 387:\n",
      "Prev Loss: 12.447964668273926\n",
      "Loss: 12.447964668273926\n",
      "Step 388:\n",
      "Prev Loss: 12.43948745727539\n",
      "Loss: 12.43948745727539\n",
      "Step 389:\n",
      "Prev Loss: 12.370811462402344\n",
      "Loss: 12.370811462402344\n",
      "Step 390:\n",
      "Prev Loss: 12.374898910522461\n",
      "Loss: 12.374898910522461\n",
      "Step 391:\n",
      "Prev Loss: 12.637775421142578\n",
      "Loss: 12.637775421142578\n",
      "Step 392:\n",
      "Prev Loss: 12.875200271606445\n",
      "Loss: 12.875200271606445\n",
      "Step 393:\n",
      "Prev Loss: 13.190455436706543\n",
      "Loss: 13.190455436706543\n",
      "Step 394:\n",
      "Prev Loss: 13.250569343566895\n",
      "Loss: 13.250569343566895\n",
      "Step 395:\n",
      "Prev Loss: 14.330364227294922\n",
      "Loss: 14.330364227294922\n",
      "Step 396:\n",
      "Prev Loss: 49.0704345703125\n",
      "Loss: 49.0704345703125\n",
      "Step 397:\n",
      "Prev Loss: 22.916542053222656\n",
      "Loss: 22.916542053222656\n",
      "Step 398:\n",
      "Prev Loss: 16.69277000427246\n",
      "Loss: 16.69277000427246\n",
      "Step 399:\n",
      "Prev Loss: 15.057304382324219\n",
      "Loss: 15.057304382324219\n",
      "Step 400:\n",
      "Prev Loss: 14.608222961425781\n",
      "Loss: 14.608222961425781\n",
      "Step 401:\n",
      "Prev Loss: 14.038806915283203\n",
      "Loss: 14.038806915283203\n",
      "Step 402:\n",
      "Prev Loss: 13.92171859741211\n",
      "Loss: 13.92171859741211\n",
      "Step 403:\n",
      "Prev Loss: 13.651113510131836\n",
      "Loss: 13.651113510131836\n",
      "Step 404:\n",
      "Prev Loss: 13.386109352111816\n",
      "Loss: 13.386109352111816\n",
      "Step 405:\n",
      "Prev Loss: 13.279708862304688\n",
      "Loss: 13.279708862304688\n",
      "Step 406:\n",
      "Prev Loss: 13.209254264831543\n",
      "Loss: 13.209254264831543\n",
      "Step 407:\n",
      "Prev Loss: 13.083505630493164\n",
      "Loss: 13.083505630493164\n",
      "Step 408:\n",
      "Prev Loss: 12.980642318725586\n",
      "Loss: 12.980642318725586\n",
      "Step 409:\n",
      "Prev Loss: 12.905194282531738\n",
      "Loss: 12.905194282531738\n",
      "Step 410:\n",
      "Prev Loss: 12.84312629699707\n",
      "Loss: 12.84312629699707\n",
      "Step 411:\n",
      "Prev Loss: 12.770991325378418\n",
      "Loss: 12.770991325378418\n",
      "Step 412:\n",
      "Prev Loss: 12.73145580291748\n",
      "Loss: 12.73145580291748\n",
      "Step 413:\n",
      "Prev Loss: 12.70219612121582\n",
      "Loss: 12.70219612121582\n",
      "Step 414:\n",
      "Prev Loss: 12.663219451904297\n",
      "Loss: 12.663219451904297\n",
      "Step 415:\n",
      "Prev Loss: 12.632576942443848\n",
      "Loss: 12.632576942443848\n",
      "Step 416:\n",
      "Prev Loss: 12.677044868469238\n",
      "Loss: 12.677044868469238\n",
      "Step 417:\n",
      "Prev Loss: 12.736096382141113\n",
      "Loss: 12.736096382141113\n",
      "Step 418:\n",
      "Prev Loss: 12.730847358703613\n",
      "Loss: 12.730847358703613\n",
      "Step 419:\n",
      "Prev Loss: 12.69830322265625\n",
      "Loss: 12.69830322265625\n",
      "Step 420:\n",
      "Prev Loss: 12.692394256591797\n",
      "Loss: 12.692394256591797\n",
      "Step 421:\n",
      "Prev Loss: 12.935530662536621\n",
      "Loss: 12.935530662536621\n",
      "Step 422:\n",
      "Prev Loss: 14.271858215332031\n",
      "Loss: 14.271858215332031\n",
      "Step 423:\n",
      "Prev Loss: 14.576323509216309\n",
      "Loss: 14.576323509216309\n",
      "Step 424:\n",
      "Prev Loss: 13.608219146728516\n",
      "Loss: 13.608219146728516\n",
      "Step 425:\n",
      "Prev Loss: 14.16724967956543\n",
      "Loss: 14.16724967956543\n",
      "Step 426:\n",
      "Prev Loss: 13.612794876098633\n",
      "Loss: 13.612794876098633\n",
      "Step 427:\n",
      "Prev Loss: 13.617420196533203\n",
      "Loss: 13.617420196533203\n",
      "Step 428:\n",
      "Prev Loss: 14.0325288772583\n",
      "Loss: 14.0325288772583\n",
      "Step 429:\n",
      "Prev Loss: 17.65045166015625\n",
      "Loss: 17.65045166015625\n",
      "Step 430:\n",
      "Prev Loss: 13.87755012512207\n",
      "Loss: 13.87755012512207\n",
      "Step 431:\n",
      "Prev Loss: 13.361479759216309\n",
      "Loss: 13.361479759216309\n",
      "Step 432:\n",
      "Prev Loss: 13.304243087768555\n",
      "Loss: 13.304243087768555\n",
      "Step 433:\n",
      "Prev Loss: 13.313404083251953\n",
      "Loss: 13.313404083251953\n",
      "Step 434:\n",
      "Prev Loss: 13.827333450317383\n",
      "Loss: 13.827333450317383\n",
      "Step 435:\n",
      "Prev Loss: 16.497440338134766\n",
      "Loss: 16.497440338134766\n",
      "Step 436:\n",
      "Prev Loss: 20.520904541015625\n",
      "Loss: 20.520904541015625\n",
      "Step 437:\n",
      "Prev Loss: 43.04331970214844\n",
      "Loss: 43.04331970214844\n",
      "Step 438:\n",
      "Prev Loss: 45.25141143798828\n",
      "Loss: 45.25141143798828\n",
      "Step 439:\n",
      "Prev Loss: 31.040578842163086\n",
      "Loss: 31.040578842163086\n",
      "Step 440:\n",
      "Prev Loss: 52.568016052246094\n",
      "Loss: 52.568016052246094\n",
      "Step 441:\n",
      "Prev Loss: 93.00958251953125\n",
      "Loss: 93.00958251953125\n",
      "Step 442:\n",
      "Prev Loss: 2483.435546875\n",
      "Loss: 2483.435546875\n",
      "Step 443:\n",
      "Prev Loss: 1099.0152587890625\n",
      "Loss: 1099.0152587890625\n",
      "Step 444:\n",
      "Prev Loss: 1636.341796875\n",
      "Loss: 1636.341796875\n",
      "Step 445:\n",
      "Prev Loss: 4426.03564453125\n",
      "Loss: 4426.03564453125\n",
      "Step 446:\n",
      "Prev Loss: 8302.46875\n",
      "Loss: 8302.46875\n",
      "Step 447:\n",
      "Prev Loss: 48584.7421875\n",
      "Loss: 48584.7421875\n",
      "Step 448:\n",
      "Prev Loss: 93194.65625\n",
      "Loss: 93194.65625\n",
      "Step 449:\n",
      "Prev Loss: 1371397.25\n",
      "Loss: 1371397.25\n",
      "Step 450:\n",
      "Prev Loss: 1821935.375\n",
      "Loss: 1821935.375\n",
      "Step 451:\n",
      "Prev Loss: 2491460.0\n",
      "Loss: 2491460.0\n",
      "Step 452:\n",
      "Prev Loss: 1883963.5\n",
      "Loss: 1883963.5\n",
      "Step 453:\n",
      "Prev Loss: 2323932.25\n",
      "Loss: 2323932.25\n",
      "Step 454:\n",
      "Prev Loss: 3041607.75\n",
      "Loss: 3041607.75\n",
      "Step 455:\n",
      "Prev Loss: 3353473.75\n",
      "Loss: 3353473.75\n",
      "Step 456:\n",
      "Prev Loss: 3383465.0\n",
      "Loss: 3383465.0\n",
      "Step 457:\n",
      "Prev Loss: 3586751.0\n",
      "Loss: 3586751.0\n",
      "Step 458:\n",
      "Prev Loss: 3258709.75\n",
      "Loss: 3258709.75\n",
      "Step 459:\n",
      "Prev Loss: 3416019.25\n",
      "Loss: 3416019.25\n",
      "Step 460:\n",
      "Prev Loss: 3649513.25\n",
      "Loss: 3649513.25\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.LBFGS([input_img])\n",
    "style_weight = 1e5\n",
    "content_weight = 1.5\n",
    "steps = 0\n",
    "prev_loss = 0\n",
    "\n",
    "while steps <= 1000 and prev_loss < 100:\n",
    "    def closure():\n",
    "        with torch.no_grad():\n",
    "            input_img.clamp_(0, 1)\n",
    "        global steps\n",
    "        global prev_loss\n",
    "        optimizer.zero_grad()\n",
    "        model(input_img)\n",
    "        content_loss = 0\n",
    "        style_loss = 0\n",
    "        for l in content_losses:\n",
    "            content_loss += l.loss\n",
    "        for l in style_losses:\n",
    "            style_loss += l.loss\n",
    "        loss = content_weight * content_loss + style_weight * style_loss\n",
    "        loss.backward()\n",
    "        steps += 1\n",
    "        if steps % 1 == 0:\n",
    "            print(f'Step {steps}:')\n",
    "            print(f'Prev Loss: {loss}')\n",
    "            print(f'Loss: {loss}')\n",
    "            save_image(input_img, f'images/output_{steps}.jpg')\n",
    "        prev_loss = loss\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    input_img.clamp_(0, 1)\n",
    "    \n",
    "save_image(input_img, 'images/output_final.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4835513d",
   "metadata": {},
   "source": [
    "And we can obtain a baby grodu image that is the same style as a [Andy Warhol](https://en.wikipedia.org/wiki/Andy_Warhol) Painting:\n",
    "\n",
    "\n",
    "Style Image                   | Transfer Image   | Content Image   \n",
    ":-------------------------:|:-------------------------:|:-------------------------:\n",
    "<img src=\"images/andyw.jpg\"  width=\"256\" height=\"256\">  |  ![](images/output_700.jpg)  | <img src=\"images/original.jpeg\"  width=\"256\" height=\"256\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c8edb0",
   "metadata": {},
   "source": [
    "## 5. AI Distributed Training with Determined AI\n",
    "\n",
    "Next, we use [determined](https://github.com/determined-ai/determined) package to enable distributed model training across clusters for more efficient training process and performance optimization. \n",
    "\n",
    "There are two ways I tried to initialize a determined AI project: WEB cluster interface vs Docker. The determined Version used in this project is:\n",
    "```shell\n",
    "$ det --version\n",
    "det 0.21.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9b360c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'determined'...\n",
      "remote: Enumerating objects: 126324, done.\u001b[K\n",
      "remote: Counting objects: 100% (1477/1477), done.\u001b[K\n",
      "remote: Compressing objects: 100% (740/740), done.\u001b[K\n",
      "remote: Total 126324 (delta 889), reused 1183 (delta 707), pack-reused 124847\u001b[K\n",
      "Receiving objects: 100% (126324/126324), 121.64 MiB | 29.15 MiB/s, done.\n",
      "Resolving deltas: 100% (97207/97207), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/determined-ai/determined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612b298",
   "metadata": {},
   "source": [
    "Following we will use **PyTorchTrial** based on [Determined AI PyTorch Trail](https://docs.determined.ai/latest/training/apis-howto/api-pytorch-ug.html) for specifying experiment configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b29c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union, Sequence\n",
    "from determined.pytorch import DataLoader, PyTorchTrial, PyTorchTrialContext\n",
    "import os\n",
    "\n",
    "TorchData = Union[Dict[str, torch.Tensor], Sequence[torch.Tensor], torch.Tensor]\n",
    "\n",
    "class MyTrial(PyTorchTrial):\n",
    "    def __init__(self, context: PyTorchTrialContext) -> None:\n",
    "        self.context = context\n",
    "        self.optimizer = optim.LBFGS([input_img])\n",
    "        \n",
    "    def build_model(self):\n",
    "        model, content_losses, style_losses = get_model_and_losses(\n",
    "    content_img, style_img, default_content_layers, default_style_layers)\n",
    "        return model\n",
    "    \n",
    "    def build_training_data_loader(self) -> DataLoader:\n",
    "        traindir = os.path.join(os.getcwd(), 'images')\n",
    "        self.normalize = Normalization()\n",
    "\n",
    "        train_dataset = datasets.ImageFolder(\n",
    "            traindir,\n",
    "            transforms.Compose([\n",
    "                transforms.RandomResizedCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                self.normalize,\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        train_loader = determined.pytorch.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.context.get_per_slot_batch_size(),\n",
    "            shuffle=True,\n",
    "            num_workers=self.context.get_hparam(\"workers\", pin_memory=True),\n",
    "        )\n",
    "        \n",
    "        return train_loader\n",
    "\n",
    "    def build_validation_data_loader(self) -> DataLoader:\n",
    "        return DataLoader()\n",
    "\n",
    "    def train_batch(self, batch: TorchData, epoch_idx: int, batch_idx: int)  -> Dict[str, Any]:\n",
    "        return {}\n",
    "\n",
    "    def evaluate_batch(self, batch: TorchData) -> Dict[str, Any]:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd0ec564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    with det.pytorch.init() as train_context:\n",
    "        trial = MyTrial(train_context)\n",
    "        trainer = det.pytorch.Trainer(trial, train_context)\n",
    "        trainer.fit(\n",
    "            max_length=pytorch.Epoch(1),\n",
    "            checkpoint_period=pytorch.Batch(500),\n",
    "            validation_period=pytorch.Batch(500),\n",
    "            checkpoint_policy=\"all\",\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO, format=det.LOG_FORMAT)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01c1664",
   "metadata": {},
   "source": [
    "Use **local distributed training**, function initialization becomes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "89b6523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dist.init_process_group(backend=\"gloo|nccl\")  \n",
    "    os.environ[\"USE_TORCH_DISTRIBUTED\"] = \"true\"\n",
    "    \n",
    "    with det.pytorch.init(distributed=core.DistributedContext.from_torch_distributed()) as train_context:\n",
    "        trial = MyTrial(train_context)\n",
    "        trainer = det.pytorch.Trainer(trial, train_context)\n",
    "        trainer.fit(\n",
    "            max_length=pytorch.Epoch(1),\n",
    "            checkpoint_period=pytorch.Batch(100),\n",
    "            validation_period=pytorch.Batch(100),\n",
    "            checkpoint_policy=\"all\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c364ee4",
   "metadata": {},
   "source": [
    "To train the model, whether it's single instance or distributed, we can start using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a037467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model=[\"name:image_style_transfer\",\n",
    "              \"global_batch_size: 32\",\n",
    "               \"dense1: 128\",\n",
    "               \"name: single\",\n",
    "               \"metric: val_accuracy\",\n",
    "               \"epochs: 5\",\n",
    "               \"entrypoint: model_def:image_style_transfer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e4ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7aaefc1",
   "metadata": {},
   "source": [
    "## 6. References\n",
    "\n",
    "- [PyTorch](https://pytorch.org/) including [TorchVision](https://pytorch.org/vision/)\n",
    "- [Basic Setup - Determined AI Documentation](https://docs.determined.ai/latest/cluster-setup-guide/basic.html)\n",
    "- [Install Determined Using Docker - DEtermined AI Documentation](https://www.googleadservices.com/pagead/aclk?sa=L&ai=DChcSEwjr-L31k7D-AhVFLK0GHcQMDlYYABAAGgJwdg&ohost=www.google.com&cid=CAESauD2a8aozeJBdbuvL_0G3p8oPtY-Z57CWfOI519uSb4QLuOJapvAsKL-hTA_kYiMZENmLoC-ZB4b7Y-QwvCWwrdlsfCE-nW2ATpYJhG8Id7saqRay842BZHpfXkN7T7PpmLaVUa0OYGjn9c&sig=AOD64_0HSgtNM8ymy1FKpSbmmeQorcDIdw&q&adurl&ved=2ahUKEwjWnrb1k7D-AhVQhu4BHciYBWkQ0Qx6BAgHEAE)\n",
    "- [Image Style Transfer Using Convolutional Neural Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)\n",
    "- [A Keras Implementation of Image Style Transfer Using Convolutional Neural Networks, Gatys et al](https://github.com/superb20/Image-Style-Transfer-Using-Convolutional-Neural-Networks)\n",
    "- [a PyTorch implementation of Image Style Transfer Using Convolutional Neural Networks](https://github.com/ali-gtw/ImageStyleTransfer-CNN)\n",
    "- [Image Style Transfer using CNN](https://github.com/Suvoo/Image-Style-Transfer-Using-CNNs)\n",
    "- [DL Style Transfer Demos](github.com/SingleZombie/DL-Demos/tree/master/dldemos/StyleTransfer)\n",
    "- [PyTorch Style Transfer Official Tutorials](https://link.zhihu.com/?target=https%3A//pytorch.org/tutorials/advanced/neural_style_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea9b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

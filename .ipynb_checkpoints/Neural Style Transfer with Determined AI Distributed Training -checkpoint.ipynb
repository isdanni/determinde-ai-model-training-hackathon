{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ab19e65",
   "metadata": {},
   "source": [
    "# Neural Style Transfer with Determined AI Distributed Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c1a62d",
   "metadata": {},
   "source": [
    "In this notebook, we are using [Determined AI Distributed Training Platform](https://www.googleadservices.com/pagead/aclk?sa=L&ai=DChcSEwj0qJ2Hha3-AhXKMq0GHdKjAHsYABAAGgJwdg&ohost=www.google.com&cid=CAESauD28j3TAEQF3m2XI5muKYYFzyP5j_nYonVGVJg5j0l7ImbzKbJzE3317fwr9tHoies3u_WbAXhOvYKWOS-uhOn1TfKKWuaELUEb01YWMkU23PIzqaQO0Rc4vj4ycnsAEANVyXvnv9pTioY&sig=AOD64_2oY2aUJkycrKR4Tq71uJOV_rGjDQ&q&adurl&ved=2ahUKEwjQ9JSHha3-AhXjOX0KHSLcB8gQ0Qx6BAgFEAE) to train a customized neural style transfer CNN Model built with PyTorch to obtain style transfer results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78979561",
   "metadata": {},
   "source": [
    "## 1. Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeeb60cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: determined in /home/danni/Documents/code/venv/lib/python3.8/site-packages (0.21.1)\n",
      "Requirement already satisfied: matplotlib in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (3.7.1)\n",
      "Requirement already satisfied: packaging in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (23.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (1.24.2)\n",
      "Requirement already satisfied: psutil in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (5.9.4)\n",
      "Requirement already satisfied: pyzmq>=18.1.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (25.0.2)\n",
      "Requirement already satisfied: yogadl==0.1.4 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (0.1.4)\n",
      "Requirement already satisfied: backoff in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (1.10.0)\n",
      "Requirement already satisfied: certifi in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (2022.12.7)\n",
      "Requirement already satisfied: filelock in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (3.11.0)\n",
      "Requirement already satisfied: google-cloud-storage in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (2.8.0)\n",
      "Requirement already satisfied: hdfs>=2.2.2 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (2.7.0)\n",
      "Requirement already satisfied: lomond>=0.3.3 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (0.3.3)\n",
      "Requirement already satisfied: pathspec>=0.6.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (0.11.1)\n",
      "Requirement already satisfied: azure-core in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (1.26.4)\n",
      "Requirement already satisfied: azure-storage-blob in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (12.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (2.2.0)\n",
      "Requirement already satisfied: boto3 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (1.26.114)\n",
      "Requirement already satisfied: argcomplete>=1.9.4 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (3.0.5)\n",
      "Requirement already satisfied: gitpython>=3.1.3 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (3.1.31)\n",
      "Requirement already satisfied: pyOpenSSL>=19.1.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (23.1.1)\n",
      "Requirement already satisfied: python-dateutil in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (2.8.2)\n",
      "Requirement already satisfied: pytz in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (2023.3)\n",
      "Requirement already satisfied: tabulate>=0.8.3 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (0.9.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.15.29 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (0.17.21)\n",
      "Requirement already satisfied: docker[ssh]>=3.7.3 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (6.0.1)\n",
      "Requirement already satisfied: google-api-python-client>=1.12.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (2.85.0)\n",
      "Requirement already satisfied: paramiko>=2.4.2 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (3.1.0)\n",
      "Requirement already satisfied: docker-compose>=1.13.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (1.29.2)\n",
      "Requirement already satisfied: tqdm in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (4.65.0)\n",
      "Requirement already satisfied: appdirs in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (1.4.4)\n",
      "Requirement already satisfied: websocket-client<1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (0.59.0)\n",
      "Requirement already satisfied: analytics-python in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from determined) (1.4.post1)\n",
      "Requirement already satisfied: async-generator in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from yogadl==0.1.4->determined) (1.10)\n",
      "Requirement already satisfied: lmdb in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from yogadl==0.1.4->determined) (1.4.1)\n",
      "Requirement already satisfied: websockets>=8.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from yogadl==0.1.4->determined) (11.0.1)\n",
      "Requirement already satisfied: PyYAML<6,>=3.10 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from docker-compose>=1.13.0->determined) (5.4.1)\n",
      "Requirement already satisfied: distro<2,>=1.5.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from docker-compose>=1.13.0->determined) (1.8.0)\n",
      "Requirement already satisfied: dockerpty<1,>=0.4.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from docker-compose>=1.13.0->determined) (0.4.1)\n",
      "Requirement already satisfied: docopt<1,>=0.6.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from docker-compose>=1.13.0->determined) (0.6.2)\n",
      "Requirement already satisfied: jsonschema<4,>=2.5.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from docker-compose>=1.13.0->determined) (3.2.0)\n",
      "Requirement already satisfied: python-dotenv<1,>=0.13.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from docker-compose>=1.13.0->determined) (0.21.1)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from docker-compose>=1.13.0->determined) (2.28.2)\n",
      "Requirement already satisfied: texttable<2,>=0.9.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from docker-compose>=1.13.0->determined) (1.6.7)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from docker[ssh]>=3.7.3->determined) (1.26.15)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from gitpython>=3.1.3->determined) (4.0.10)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-api-python-client>=1.12.1->determined) (0.22.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-api-python-client>=1.12.1->determined) (2.17.3)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-api-python-client>=1.12.1->determined) (0.1.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-api-python-client>=1.12.1->determined) (2.11.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-api-python-client>=1.12.1->determined) (4.1.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from hdfs>=2.2.2->determined) (1.16.0)\n",
      "Requirement already satisfied: bcrypt>=3.2 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from paramiko>=2.4.2->determined) (4.0.1)\n",
      "Requirement already satisfied: cryptography>=3.3 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from paramiko>=2.4.2->determined) (40.0.2)\n",
      "Requirement already satisfied: pynacl>=1.5 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from paramiko>=2.4.2->determined) (1.5.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.6 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from ruamel.yaml>=0.15.29->determined) (0.2.7)\n",
      "Requirement already satisfied: monotonic>=1.5 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from analytics-python->determined) (1.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from azure-core->determined) (4.5.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from azure-storage-blob->determined) (0.6.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.114 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from boto3->determined) (1.29.114)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from boto3->determined) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from boto3->determined) (0.6.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-cloud-storage->determined) (2.3.2)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-cloud-storage->determined) (2.4.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from matplotlib->determined) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from matplotlib->determined) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from matplotlib->determined) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from matplotlib->determined) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from matplotlib->determined) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from matplotlib->determined) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from matplotlib->determined) (5.12.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from cryptography>=3.3->paramiko>=2.4.2->determined) (1.15.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.3->determined) (5.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.12.1->determined) (1.59.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.12.1->determined) (4.22.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.12.1->determined) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.12.1->determined) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.12.1->determined) (4.9)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from google-resumable-media>=2.3.2->google-cloud-storage->determined) (1.5.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->determined) (3.15.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from jsonschema<4,>=2.5.1->docker-compose>=1.13.0->determined) (22.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from jsonschema<4,>=2.5.1->docker-compose>=1.13.0->determined) (0.19.3)\n",
      "Requirement already satisfied: setuptools in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from jsonschema<4,>=2.5.1->docker-compose>=1.13.0->determined) (67.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from requests<3,>=2.20.0->docker-compose>=1.13.0->determined) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from requests<3,>=2.20.0->docker-compose>=1.13.0->determined) (3.4)\n",
      "Requirement already satisfied: pycparser in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4.2->determined) (2.21)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/danni/Documents/code/venv/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.12.1->determined) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install determined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "688f2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio\n",
    "# commented out to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b799d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50159026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33d3162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# turn off warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a14b66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd9192",
   "metadata": {},
   "source": [
    "## 2. Problem Definition & Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b30689",
   "metadata": {},
   "source": [
    "Imganie creating a copy of a painting as a mathematical optimization problem. Given the source image `S`, we want to generate a target image `T`, so that the mean square error **MSE(S-T)** is the smallest, which means this imaage generation problem becomes an optimization problem of finding the optimal `T`.\n",
    "\n",
    "For this problem, we can randomly initialize an image `T`, perform gradient descent based on this optimization goal, and eventually find the optimal `T`, a target image that is the closest to the source image `S`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9699be7b",
   "metadata": {},
   "source": [
    "![model-architecture](images/model-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a04689",
   "metadata": {},
   "source": [
    "paper link: [Image Style Transfer Using Convolutional Neural Networks](https://openaccess.thecvf.com/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf) \n",
    "\n",
    "The expected result should be:\n",
    "\n",
    "![paper image](images/paper-image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f80ad",
   "metadata": {},
   "source": [
    "## 3. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481b7c7",
   "metadata": {},
   "source": [
    "In order to calculate MSE correctly, the shape of all images must be **uniform**. Therefore, we resize images to `(256, 256)`. \n",
    "\n",
    "After data preprocessing, the image format is in `(c, h, w)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae9a900",
   "metadata": {},
   "source": [
    "Style Image            |  Content Image\n",
    ":-------------------------:|:-------------------------:\n",
    "![](images/andyw.jpg)  |  ![](images/grogu.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eff22088",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d950313",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (256, 256)\n",
    "\n",
    "\n",
    "def read_image(image_path):\n",
    "    pipeline = transforms.Compose([transforms.Resize((img_size)), transforms.ToTensor()])\n",
    "\n",
    "    img = Image.open(image_path)\n",
    "    img = pipeline(img).unsqueeze(0)\n",
    "    return img.to(device, torch.float)\n",
    "\n",
    "\n",
    "def save_image(tensor, image_path):\n",
    "    toPIL = transforms.ToPILImage()\n",
    "    \n",
    "    img = tensor.detach().cpu().clone()\n",
    "    img = toPIL(img.squeeze(0))\n",
    "    \n",
    "    img.save(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72801f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_img = read_image('images/andyw.jpg')\n",
    "content_img = read_image('images/grogu.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fc61835",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = torch.randn(1, 3, *img_size, device=device)\n",
    "input_img.requires_grad_(True)\n",
    "optimizer = optim.LBFGS([input_img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a48ae162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "Loss: 2.545506477355957\n",
      "Step 2:\n",
      "Loss: 2.545478343963623\n",
      "Step 3:\n",
      "Loss: 0.11352673172950745\n",
      "Step 4:\n",
      "Loss: 0.11331775039434433\n",
      "Step 5:\n",
      "Loss: 0.11331775039434433\n",
      "Step 6:\n",
      "Loss: 0.11331775039434433\n",
      "Step 7:\n",
      "Loss: 0.11331775039434433\n",
      "Step 8:\n",
      "Loss: 0.11331775039434433\n",
      "Step 9:\n",
      "Loss: 0.11331775039434433\n",
      "Step 10:\n",
      "Loss: 0.11331775039434433\n",
      "Step 11:\n",
      "Loss: 0.11331775039434433\n"
     ]
    }
   ],
   "source": [
    "steps = 0\n",
    "while steps <= 10:\n",
    "    def closure():\n",
    "        global steps\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.mse_loss(input_img, style_img) + F.mse_loss(input_img, content_img)\n",
    "        loss.backward()\n",
    "        \n",
    "        steps += 1\n",
    "        if steps % 1 == 0:\n",
    "            print(f'Step {steps}:')\n",
    "            print(f'Loss: {loss}')\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "\n",
    "save_image(input_img, 'images/output.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1eedea",
   "metadata": {},
   "source": [
    "The output picture now looks like this: ![out](images/output.jpg). We will be reusing `read_image` and `save_image` in the following model training tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6654e91",
   "metadata": {},
   "source": [
    "## 4. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157f3e5",
   "metadata": {},
   "source": [
    "### 4.1. Loss Function\n",
    "Since we are using MSE to cauculate loss in this notebook, we will reuse its `F.mse_loss` directly instead of defining our own `forward()` functions in the `torch.autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05f74ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(x: torch.Tensor):\n",
    "    n, c, h, w = x.shape\n",
    "\n",
    "    features = x.reshape(n * c, h * w)\n",
    "    features = torch.mm(features, features.T) / n / c / h / w\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67406ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentLoss(torch.nn.Module):\n",
    "    def __init__(self, target: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input\n",
    "    \n",
    "\n",
    "class StyleLoss(torch.nn.Module):\n",
    "    def __init__(self, target: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.target = features(target.detach()).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = features(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4073a75",
   "metadata": {},
   "source": [
    "### 4.2. Normalization\n",
    "\n",
    "We also use one layer to normalize input distribution so the mean and standard deviation of the input data can be directly used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61b9d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization(torch.nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super().__init__()\n",
    "        self.mean = torch.tensor(mean).to(device).reshape(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).to(device).reshape(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a8bb2b",
   "metadata": {},
   "source": [
    "### 4.3. Model Assemble \n",
    "\n",
    "Following, we assemble the norm layer, the loss calculation and features(ONLY) of a pretrained VGG model from `torch.vision` using `torch.nn.Sequential`. If a certain layer is used for MSE calculation, we attach the loss module to it. \n",
    "\n",
    "In the following log, we can see the model architecture consists of:\n",
    "\n",
    "```\n",
    " norm => conv_1 => style_loss_1 => relu_1 => conv_2 => style_loss_2 => relu_2 => pool_2 => conv_3 => style_loss_3 => relu_3 => conv_4 => content_loss_4 => style_loss_4 => relu_4 => pool_4 => conv_5 => style_loss_5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23c7abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_content_layers = ['conv_4']\n",
    "default_style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "def get_model_and_losses(content_img, style_img, content_layers, style_layers):\n",
    "    num_loss = 0\n",
    "    expected_num_loss = len(content_layers) + len(style_layers)\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    model = torch.nn.Sequential(\n",
    "        Normalization([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]))\n",
    "    cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "    i = 0\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, torch.nn.Conv2d):\n",
    "            i += 1\n",
    "            name = f'conv_{i}'\n",
    "        elif isinstance(layer, torch.nn.ReLU):\n",
    "            name = f'relu_{i}'\n",
    "            layer = torch.nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, torch.nn.MaxPool2d):\n",
    "            name = f'pool_{i}'\n",
    "        elif isinstance(layer, torch.nn.BatchNorm2d):\n",
    "            name = f'bn_{i}'\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f'Unrecognized layer: {layer.__class__.__name__}')\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            target = model(content_img)\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(f'content_loss_{i}', content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "            num_loss += 1\n",
    "\n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_img)\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(f'style_loss_{i}', style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "            num_loss += 1\n",
    "\n",
    "        if num_loss >= expected_num_loss:\n",
    "            break\n",
    "\n",
    "    return model, content_losses, style_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3caaaf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_img = read_image('images/andyw.jpg')\n",
    "content_img = read_image('images/grogu.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ec180ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Normalization()\n",
       "  (conv_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (style_loss_1): StyleLoss()\n",
       "  (relu_1): ReLU()\n",
       "  (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (style_loss_2): StyleLoss()\n",
       "  (relu_2): ReLU()\n",
       "  (pool_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (style_loss_3): StyleLoss()\n",
       "  (relu_3): ReLU()\n",
       "  (conv_4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (content_loss_4): ContentLoss()\n",
       "  (style_loss_4): StyleLoss()\n",
       "  (relu_4): ReLU()\n",
       "  (pool_4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (style_loss_5): StyleLoss()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img = torch.randn(1, 3, *img_size, device=device)\n",
    "model, content_losses, style_losses = get_model_and_losses(\n",
    "    content_img, style_img, default_content_layers, default_style_layers)\n",
    "\n",
    "input_img.requires_grad_(True)\n",
    "model.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d49d43",
   "metadata": {},
   "source": [
    "### 4.4. Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc2b93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "Prev Loss: 138.18373107910156\n",
      "Loss: 138.18373107910156\n",
      "Step 2:\n",
      "Prev Loss: 138.14935302734375\n",
      "Loss: 138.14935302734375\n",
      "Step 3:\n",
      "Prev Loss: 134.3802490234375\n",
      "Loss: 134.3802490234375\n",
      "Step 4:\n",
      "Prev Loss: 131.07571411132812\n",
      "Loss: 131.07571411132812\n",
      "Step 5:\n",
      "Prev Loss: 129.2333984375\n",
      "Loss: 129.2333984375\n",
      "Step 6:\n",
      "Prev Loss: 125.85127258300781\n",
      "Loss: 125.85127258300781\n",
      "Step 7:\n",
      "Prev Loss: 122.00393676757812\n",
      "Loss: 122.00393676757812\n",
      "Step 8:\n",
      "Prev Loss: 119.15552520751953\n",
      "Loss: 119.15552520751953\n",
      "Step 9:\n",
      "Prev Loss: 115.63438415527344\n",
      "Loss: 115.63438415527344\n",
      "Step 10:\n",
      "Prev Loss: 112.48789978027344\n",
      "Loss: 112.48789978027344\n",
      "Step 11:\n",
      "Prev Loss: 109.19090270996094\n",
      "Loss: 109.19090270996094\n",
      "Step 12:\n",
      "Prev Loss: 105.84829711914062\n",
      "Loss: 105.84829711914062\n",
      "Step 13:\n",
      "Prev Loss: 102.75071716308594\n",
      "Loss: 102.75071716308594\n",
      "Step 14:\n",
      "Prev Loss: 99.83247375488281\n",
      "Loss: 99.83247375488281\n",
      "Step 15:\n",
      "Prev Loss: 96.95794677734375\n",
      "Loss: 96.95794677734375\n",
      "Step 16:\n",
      "Prev Loss: 94.36836242675781\n",
      "Loss: 94.36836242675781\n",
      "Step 17:\n",
      "Prev Loss: 92.01255798339844\n",
      "Loss: 92.01255798339844\n",
      "Step 18:\n",
      "Prev Loss: 89.79371643066406\n",
      "Loss: 89.79371643066406\n",
      "Step 19:\n",
      "Prev Loss: 87.7693099975586\n",
      "Loss: 87.7693099975586\n",
      "Step 20:\n",
      "Prev Loss: 86.11137390136719\n",
      "Loss: 86.11137390136719\n",
      "Step 21:\n",
      "Prev Loss: 84.82090759277344\n",
      "Loss: 84.82090759277344\n",
      "Step 22:\n",
      "Prev Loss: 83.07844543457031\n",
      "Loss: 83.07844543457031\n",
      "Step 23:\n",
      "Prev Loss: 81.55155944824219\n",
      "Loss: 81.55155944824219\n",
      "Step 24:\n",
      "Prev Loss: 80.22563171386719\n",
      "Loss: 80.22563171386719\n",
      "Step 25:\n",
      "Prev Loss: 78.88179779052734\n",
      "Loss: 78.88179779052734\n",
      "Step 26:\n",
      "Prev Loss: 77.43681335449219\n",
      "Loss: 77.43681335449219\n",
      "Step 27:\n",
      "Prev Loss: 75.97700500488281\n",
      "Loss: 75.97700500488281\n",
      "Step 28:\n",
      "Prev Loss: 74.68473052978516\n",
      "Loss: 74.68473052978516\n",
      "Step 29:\n",
      "Prev Loss: 73.49797058105469\n",
      "Loss: 73.49797058105469\n",
      "Step 30:\n",
      "Prev Loss: 72.28141784667969\n",
      "Loss: 72.28141784667969\n",
      "Step 31:\n",
      "Prev Loss: 71.17313385009766\n",
      "Loss: 71.17313385009766\n",
      "Step 32:\n",
      "Prev Loss: 70.13068389892578\n",
      "Loss: 70.13068389892578\n",
      "Step 33:\n",
      "Prev Loss: 69.20713806152344\n",
      "Loss: 69.20713806152344\n",
      "Step 34:\n",
      "Prev Loss: 68.17747497558594\n",
      "Loss: 68.17747497558594\n",
      "Step 35:\n",
      "Prev Loss: 67.13361358642578\n",
      "Loss: 67.13361358642578\n",
      "Step 36:\n",
      "Prev Loss: 66.08409118652344\n",
      "Loss: 66.08409118652344\n",
      "Step 37:\n",
      "Prev Loss: 65.20887756347656\n",
      "Loss: 65.20887756347656\n",
      "Step 38:\n",
      "Prev Loss: 64.30146789550781\n",
      "Loss: 64.30146789550781\n",
      "Step 39:\n",
      "Prev Loss: 63.4135627746582\n",
      "Loss: 63.4135627746582\n",
      "Step 40:\n",
      "Prev Loss: 62.63046646118164\n",
      "Loss: 62.63046646118164\n",
      "Step 41:\n",
      "Prev Loss: 61.7912483215332\n",
      "Loss: 61.7912483215332\n",
      "Step 42:\n",
      "Prev Loss: 60.9219856262207\n",
      "Loss: 60.9219856262207\n",
      "Step 43:\n",
      "Prev Loss: 60.1462516784668\n",
      "Loss: 60.1462516784668\n",
      "Step 44:\n",
      "Prev Loss: 59.40892028808594\n",
      "Loss: 59.40892028808594\n",
      "Step 45:\n",
      "Prev Loss: 58.67362976074219\n",
      "Loss: 58.67362976074219\n",
      "Step 46:\n",
      "Prev Loss: 57.853553771972656\n",
      "Loss: 57.853553771972656\n",
      "Step 47:\n",
      "Prev Loss: 57.06682205200195\n",
      "Loss: 57.06682205200195\n",
      "Step 48:\n",
      "Prev Loss: 56.33692169189453\n",
      "Loss: 56.33692169189453\n",
      "Step 49:\n",
      "Prev Loss: 55.64268493652344\n",
      "Loss: 55.64268493652344\n",
      "Step 50:\n",
      "Prev Loss: 54.91450881958008\n",
      "Loss: 54.91450881958008\n",
      "Step 51:\n",
      "Prev Loss: 54.22505187988281\n",
      "Loss: 54.22505187988281\n",
      "Step 52:\n",
      "Prev Loss: 53.67008972167969\n",
      "Loss: 53.67008972167969\n",
      "Step 53:\n",
      "Prev Loss: 53.16676330566406\n",
      "Loss: 53.16676330566406\n",
      "Step 54:\n",
      "Prev Loss: 52.53626251220703\n",
      "Loss: 52.53626251220703\n",
      "Step 55:\n",
      "Prev Loss: 51.95294952392578\n",
      "Loss: 51.95294952392578\n",
      "Step 56:\n",
      "Prev Loss: 51.46356964111328\n",
      "Loss: 51.46356964111328\n",
      "Step 57:\n",
      "Prev Loss: 50.93619155883789\n",
      "Loss: 50.93619155883789\n",
      "Step 58:\n",
      "Prev Loss: 50.34822082519531\n",
      "Loss: 50.34822082519531\n",
      "Step 59:\n",
      "Prev Loss: 49.8370361328125\n",
      "Loss: 49.8370361328125\n",
      "Step 60:\n",
      "Prev Loss: 49.36004638671875\n",
      "Loss: 49.36004638671875\n",
      "Step 61:\n",
      "Prev Loss: 48.941139221191406\n",
      "Loss: 48.941139221191406\n",
      "Step 62:\n",
      "Prev Loss: 48.47284698486328\n",
      "Loss: 48.47284698486328\n",
      "Step 63:\n",
      "Prev Loss: 48.031192779541016\n",
      "Loss: 48.031192779541016\n",
      "Step 64:\n",
      "Prev Loss: 47.552276611328125\n",
      "Loss: 47.552276611328125\n",
      "Step 65:\n",
      "Prev Loss: 47.130165100097656\n",
      "Loss: 47.130165100097656\n",
      "Step 66:\n",
      "Prev Loss: 46.739471435546875\n",
      "Loss: 46.739471435546875\n",
      "Step 67:\n",
      "Prev Loss: 46.34263610839844\n",
      "Loss: 46.34263610839844\n",
      "Step 68:\n",
      "Prev Loss: 45.898963928222656\n",
      "Loss: 45.898963928222656\n",
      "Step 69:\n",
      "Prev Loss: 45.57008361816406\n",
      "Loss: 45.57008361816406\n",
      "Step 70:\n",
      "Prev Loss: 45.22351837158203\n",
      "Loss: 45.22351837158203\n",
      "Step 71:\n",
      "Prev Loss: 44.8622932434082\n",
      "Loss: 44.8622932434082\n",
      "Step 72:\n",
      "Prev Loss: 44.50065612792969\n",
      "Loss: 44.50065612792969\n",
      "Step 73:\n",
      "Prev Loss: 44.12664794921875\n",
      "Loss: 44.12664794921875\n",
      "Step 74:\n",
      "Prev Loss: 43.728759765625\n",
      "Loss: 43.728759765625\n",
      "Step 75:\n",
      "Prev Loss: 43.34158706665039\n",
      "Loss: 43.34158706665039\n",
      "Step 76:\n",
      "Prev Loss: 42.95209503173828\n",
      "Loss: 42.95209503173828\n",
      "Step 77:\n",
      "Prev Loss: 42.59539794921875\n",
      "Loss: 42.59539794921875\n",
      "Step 78:\n",
      "Prev Loss: 42.21616744995117\n",
      "Loss: 42.21616744995117\n",
      "Step 79:\n",
      "Prev Loss: 41.939762115478516\n",
      "Loss: 41.939762115478516\n",
      "Step 80:\n",
      "Prev Loss: 41.51780700683594\n",
      "Loss: 41.51780700683594\n",
      "Step 81:\n",
      "Prev Loss: 41.1778564453125\n",
      "Loss: 41.1778564453125\n",
      "Step 82:\n",
      "Prev Loss: 40.92298889160156\n",
      "Loss: 40.92298889160156\n",
      "Step 83:\n",
      "Prev Loss: 40.67041015625\n",
      "Loss: 40.67041015625\n",
      "Step 84:\n",
      "Prev Loss: 40.38214874267578\n",
      "Loss: 40.38214874267578\n",
      "Step 85:\n",
      "Prev Loss: 40.068389892578125\n",
      "Loss: 40.068389892578125\n",
      "Step 86:\n",
      "Prev Loss: 39.74378967285156\n",
      "Loss: 39.74378967285156\n",
      "Step 87:\n",
      "Prev Loss: 39.421241760253906\n",
      "Loss: 39.421241760253906\n",
      "Step 88:\n",
      "Prev Loss: 39.10607147216797\n",
      "Loss: 39.10607147216797\n",
      "Step 89:\n",
      "Prev Loss: 38.80396270751953\n",
      "Loss: 38.80396270751953\n",
      "Step 90:\n",
      "Prev Loss: 38.49842071533203\n",
      "Loss: 38.49842071533203\n",
      "Step 91:\n",
      "Prev Loss: 38.25977325439453\n",
      "Loss: 38.25977325439453\n",
      "Step 92:\n",
      "Prev Loss: 37.95698547363281\n",
      "Loss: 37.95698547363281\n",
      "Step 93:\n",
      "Prev Loss: 37.6745491027832\n",
      "Loss: 37.6745491027832\n",
      "Step 94:\n",
      "Prev Loss: 37.4111442565918\n",
      "Loss: 37.4111442565918\n",
      "Step 95:\n",
      "Prev Loss: 37.20659637451172\n",
      "Loss: 37.20659637451172\n",
      "Step 96:\n",
      "Prev Loss: 36.99068069458008\n",
      "Loss: 36.99068069458008\n",
      "Step 97:\n",
      "Prev Loss: 36.75322723388672\n",
      "Loss: 36.75322723388672\n",
      "Step 98:\n",
      "Prev Loss: 36.48619079589844\n",
      "Loss: 36.48619079589844\n",
      "Step 99:\n",
      "Prev Loss: 36.25584030151367\n",
      "Loss: 36.25584030151367\n",
      "Step 100:\n",
      "Prev Loss: 35.996463775634766\n",
      "Loss: 35.996463775634766\n",
      "Step 101:\n",
      "Prev Loss: 35.76469421386719\n",
      "Loss: 35.76469421386719\n",
      "Step 102:\n",
      "Prev Loss: 35.49951171875\n",
      "Loss: 35.49951171875\n",
      "Step 103:\n",
      "Prev Loss: 35.266578674316406\n",
      "Loss: 35.266578674316406\n",
      "Step 104:\n",
      "Prev Loss: 35.0274772644043\n",
      "Loss: 35.0274772644043\n",
      "Step 105:\n",
      "Prev Loss: 34.811248779296875\n",
      "Loss: 34.811248779296875\n",
      "Step 106:\n",
      "Prev Loss: 34.596614837646484\n",
      "Loss: 34.596614837646484\n",
      "Step 107:\n",
      "Prev Loss: 34.39703369140625\n",
      "Loss: 34.39703369140625\n",
      "Step 108:\n",
      "Prev Loss: 34.25959777832031\n",
      "Loss: 34.25959777832031\n",
      "Step 109:\n",
      "Prev Loss: 33.975948333740234\n",
      "Loss: 33.975948333740234\n",
      "Step 110:\n",
      "Prev Loss: 33.79289627075195\n",
      "Loss: 33.79289627075195\n",
      "Step 111:\n",
      "Prev Loss: 33.6212158203125\n",
      "Loss: 33.6212158203125\n",
      "Step 112:\n",
      "Prev Loss: 33.42518615722656\n",
      "Loss: 33.42518615722656\n",
      "Step 113:\n",
      "Prev Loss: 33.2077522277832\n",
      "Loss: 33.2077522277832\n",
      "Step 114:\n",
      "Prev Loss: 32.998313903808594\n",
      "Loss: 32.998313903808594\n",
      "Step 115:\n",
      "Prev Loss: 32.77167510986328\n",
      "Loss: 32.77167510986328\n",
      "Step 116:\n",
      "Prev Loss: 32.581932067871094\n",
      "Loss: 32.581932067871094\n",
      "Step 117:\n",
      "Prev Loss: 32.373538970947266\n",
      "Loss: 32.373538970947266\n",
      "Step 118:\n",
      "Prev Loss: 32.168365478515625\n",
      "Loss: 32.168365478515625\n",
      "Step 119:\n",
      "Prev Loss: 31.962984085083008\n",
      "Loss: 31.962984085083008\n",
      "Step 120:\n",
      "Prev Loss: 31.811058044433594\n",
      "Loss: 31.811058044433594\n",
      "Step 121:\n",
      "Prev Loss: 31.64769172668457\n",
      "Loss: 31.64769172668457\n",
      "Step 122:\n",
      "Prev Loss: 31.469871520996094\n",
      "Loss: 31.469871520996094\n",
      "Step 123:\n",
      "Prev Loss: 31.32598304748535\n",
      "Loss: 31.32598304748535\n",
      "Step 124:\n",
      "Prev Loss: 31.142799377441406\n",
      "Loss: 31.142799377441406\n",
      "Step 125:\n",
      "Prev Loss: 30.990943908691406\n",
      "Loss: 30.990943908691406\n",
      "Step 126:\n",
      "Prev Loss: 30.772621154785156\n",
      "Loss: 30.772621154785156\n",
      "Step 127:\n",
      "Prev Loss: 30.62862777709961\n",
      "Loss: 30.62862777709961\n",
      "Step 128:\n",
      "Prev Loss: 30.43539810180664\n",
      "Loss: 30.43539810180664\n",
      "Step 129:\n",
      "Prev Loss: 30.262737274169922\n",
      "Loss: 30.262737274169922\n",
      "Step 130:\n",
      "Prev Loss: 30.117694854736328\n",
      "Loss: 30.117694854736328\n",
      "Step 131:\n",
      "Prev Loss: 29.95729637145996\n",
      "Loss: 29.95729637145996\n",
      "Step 132:\n",
      "Prev Loss: 29.772415161132812\n",
      "Loss: 29.772415161132812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 133:\n",
      "Prev Loss: 29.603731155395508\n",
      "Loss: 29.603731155395508\n",
      "Step 134:\n",
      "Prev Loss: 29.442094802856445\n",
      "Loss: 29.442094802856445\n",
      "Step 135:\n",
      "Prev Loss: 29.3131103515625\n",
      "Loss: 29.3131103515625\n",
      "Step 136:\n",
      "Prev Loss: 29.14924430847168\n",
      "Loss: 29.14924430847168\n",
      "Step 137:\n",
      "Prev Loss: 28.995487213134766\n",
      "Loss: 28.995487213134766\n",
      "Step 138:\n",
      "Prev Loss: 28.809207916259766\n",
      "Loss: 28.809207916259766\n",
      "Step 139:\n",
      "Prev Loss: 28.659072875976562\n",
      "Loss: 28.659072875976562\n",
      "Step 140:\n",
      "Prev Loss: 28.47382354736328\n",
      "Loss: 28.47382354736328\n",
      "Step 141:\n",
      "Prev Loss: 28.340579986572266\n",
      "Loss: 28.340579986572266\n",
      "Step 142:\n",
      "Prev Loss: 28.14116668701172\n",
      "Loss: 28.14116668701172\n",
      "Step 143:\n",
      "Prev Loss: 28.036678314208984\n",
      "Loss: 28.036678314208984\n",
      "Step 144:\n",
      "Prev Loss: 27.905637741088867\n",
      "Loss: 27.905637741088867\n",
      "Step 145:\n",
      "Prev Loss: 27.762479782104492\n",
      "Loss: 27.762479782104492\n",
      "Step 146:\n",
      "Prev Loss: 27.63558578491211\n",
      "Loss: 27.63558578491211\n",
      "Step 147:\n",
      "Prev Loss: 27.516826629638672\n",
      "Loss: 27.516826629638672\n",
      "Step 148:\n",
      "Prev Loss: 27.358781814575195\n",
      "Loss: 27.358781814575195\n",
      "Step 149:\n",
      "Prev Loss: 27.21001434326172\n",
      "Loss: 27.21001434326172\n",
      "Step 150:\n",
      "Prev Loss: 27.08121681213379\n",
      "Loss: 27.08121681213379\n",
      "Step 151:\n",
      "Prev Loss: 26.94532012939453\n",
      "Loss: 26.94532012939453\n",
      "Step 152:\n",
      "Prev Loss: 26.821392059326172\n",
      "Loss: 26.821392059326172\n",
      "Step 153:\n",
      "Prev Loss: 26.649274826049805\n",
      "Loss: 26.649274826049805\n",
      "Step 154:\n",
      "Prev Loss: 26.548370361328125\n",
      "Loss: 26.548370361328125\n",
      "Step 155:\n",
      "Prev Loss: 26.417295455932617\n",
      "Loss: 26.417295455932617\n",
      "Step 156:\n",
      "Prev Loss: 26.283077239990234\n",
      "Loss: 26.283077239990234\n",
      "Step 157:\n",
      "Prev Loss: 26.193145751953125\n",
      "Loss: 26.193145751953125\n",
      "Step 158:\n",
      "Prev Loss: 26.031583786010742\n",
      "Loss: 26.031583786010742\n",
      "Step 159:\n",
      "Prev Loss: 25.91053009033203\n",
      "Loss: 25.91053009033203\n",
      "Step 160:\n",
      "Prev Loss: 25.776416778564453\n",
      "Loss: 25.776416778564453\n",
      "Step 161:\n",
      "Prev Loss: 25.642597198486328\n",
      "Loss: 25.642597198486328\n",
      "Step 162:\n",
      "Prev Loss: 25.51754379272461\n",
      "Loss: 25.51754379272461\n",
      "Step 163:\n",
      "Prev Loss: 25.364009857177734\n",
      "Loss: 25.364009857177734\n",
      "Step 164:\n",
      "Prev Loss: 25.2677001953125\n",
      "Loss: 25.2677001953125\n",
      "Step 165:\n",
      "Prev Loss: 25.123462677001953\n",
      "Loss: 25.123462677001953\n",
      "Step 166:\n",
      "Prev Loss: 25.018524169921875\n",
      "Loss: 25.018524169921875\n",
      "Step 167:\n",
      "Prev Loss: 24.898372650146484\n",
      "Loss: 24.898372650146484\n",
      "Step 168:\n",
      "Prev Loss: 24.802215576171875\n",
      "Loss: 24.802215576171875\n",
      "Step 169:\n",
      "Prev Loss: 24.687528610229492\n",
      "Loss: 24.687528610229492\n",
      "Step 170:\n",
      "Prev Loss: 24.57345962524414\n",
      "Loss: 24.57345962524414\n",
      "Step 171:\n",
      "Prev Loss: 24.455913543701172\n",
      "Loss: 24.455913543701172\n",
      "Step 172:\n",
      "Prev Loss: 24.30075454711914\n",
      "Loss: 24.30075454711914\n",
      "Step 173:\n",
      "Prev Loss: 24.223602294921875\n",
      "Loss: 24.223602294921875\n",
      "Step 174:\n",
      "Prev Loss: 24.092914581298828\n",
      "Loss: 24.092914581298828\n",
      "Step 175:\n",
      "Prev Loss: 23.970388412475586\n",
      "Loss: 23.970388412475586\n",
      "Step 176:\n",
      "Prev Loss: 23.848222732543945\n",
      "Loss: 23.848222732543945\n",
      "Step 177:\n",
      "Prev Loss: 23.780006408691406\n",
      "Loss: 23.780006408691406\n",
      "Step 178:\n",
      "Prev Loss: 23.638965606689453\n",
      "Loss: 23.638965606689453\n",
      "Step 179:\n",
      "Prev Loss: 23.551958084106445\n",
      "Loss: 23.551958084106445\n",
      "Step 180:\n",
      "Prev Loss: 23.391923904418945\n",
      "Loss: 23.391923904418945\n",
      "Step 181:\n",
      "Prev Loss: 23.30290985107422\n",
      "Loss: 23.30290985107422\n",
      "Step 182:\n",
      "Prev Loss: 23.164695739746094\n",
      "Loss: 23.164695739746094\n",
      "Step 183:\n",
      "Prev Loss: 23.027589797973633\n",
      "Loss: 23.027589797973633\n",
      "Step 184:\n",
      "Prev Loss: 22.930007934570312\n",
      "Loss: 22.930007934570312\n",
      "Step 185:\n",
      "Prev Loss: 22.830114364624023\n",
      "Loss: 22.830114364624023\n",
      "Step 186:\n",
      "Prev Loss: 22.717021942138672\n",
      "Loss: 22.717021942138672\n",
      "Step 187:\n",
      "Prev Loss: 22.60724639892578\n",
      "Loss: 22.60724639892578\n",
      "Step 188:\n",
      "Prev Loss: 22.573261260986328\n",
      "Loss: 22.573261260986328\n",
      "Step 189:\n",
      "Prev Loss: 22.467559814453125\n",
      "Loss: 22.467559814453125\n",
      "Step 190:\n",
      "Prev Loss: 22.408374786376953\n",
      "Loss: 22.408374786376953\n",
      "Step 191:\n",
      "Prev Loss: 22.276775360107422\n",
      "Loss: 22.276775360107422\n",
      "Step 192:\n",
      "Prev Loss: 22.218032836914062\n",
      "Loss: 22.218032836914062\n",
      "Step 193:\n",
      "Prev Loss: 22.1163387298584\n",
      "Loss: 22.1163387298584\n",
      "Step 194:\n",
      "Prev Loss: 22.002925872802734\n",
      "Loss: 22.002925872802734\n",
      "Step 195:\n",
      "Prev Loss: 21.90798568725586\n",
      "Loss: 21.90798568725586\n",
      "Step 196:\n",
      "Prev Loss: 21.80487060546875\n",
      "Loss: 21.80487060546875\n",
      "Step 197:\n",
      "Prev Loss: 21.699756622314453\n",
      "Loss: 21.699756622314453\n",
      "Step 198:\n",
      "Prev Loss: 21.604122161865234\n",
      "Loss: 21.604122161865234\n",
      "Step 199:\n",
      "Prev Loss: 21.512042999267578\n",
      "Loss: 21.512042999267578\n",
      "Step 200:\n",
      "Prev Loss: 21.43158531188965\n",
      "Loss: 21.43158531188965\n",
      "Step 201:\n",
      "Prev Loss: 21.35442352294922\n",
      "Loss: 21.35442352294922\n",
      "Step 202:\n",
      "Prev Loss: 21.239612579345703\n",
      "Loss: 21.239612579345703\n",
      "Step 203:\n",
      "Prev Loss: 21.294631958007812\n",
      "Loss: 21.294631958007812\n",
      "Step 204:\n",
      "Prev Loss: 21.125408172607422\n",
      "Loss: 21.125408172607422\n",
      "Step 205:\n",
      "Prev Loss: 21.012802124023438\n",
      "Loss: 21.012802124023438\n",
      "Step 206:\n",
      "Prev Loss: 21.03594970703125\n",
      "Loss: 21.03594970703125\n",
      "Step 207:\n",
      "Prev Loss: 20.887474060058594\n",
      "Loss: 20.887474060058594\n",
      "Step 208:\n",
      "Prev Loss: 20.790130615234375\n",
      "Loss: 20.790130615234375\n",
      "Step 209:\n",
      "Prev Loss: 20.698453903198242\n",
      "Loss: 20.698453903198242\n",
      "Step 210:\n",
      "Prev Loss: 20.64371681213379\n",
      "Loss: 20.64371681213379\n",
      "Step 211:\n",
      "Prev Loss: 20.557559967041016\n",
      "Loss: 20.557559967041016\n",
      "Step 212:\n",
      "Prev Loss: 20.43103790283203\n",
      "Loss: 20.43103790283203\n",
      "Step 213:\n",
      "Prev Loss: 20.34039306640625\n",
      "Loss: 20.34039306640625\n",
      "Step 214:\n",
      "Prev Loss: 20.264760971069336\n",
      "Loss: 20.264760971069336\n",
      "Step 215:\n",
      "Prev Loss: 20.186750411987305\n",
      "Loss: 20.186750411987305\n",
      "Step 216:\n",
      "Prev Loss: 20.09063720703125\n",
      "Loss: 20.09063720703125\n",
      "Step 217:\n",
      "Prev Loss: 20.009485244750977\n",
      "Loss: 20.009485244750977\n",
      "Step 218:\n",
      "Prev Loss: 20.019580841064453\n",
      "Loss: 20.019580841064453\n",
      "Step 219:\n",
      "Prev Loss: 19.920745849609375\n",
      "Loss: 19.920745849609375\n",
      "Step 220:\n",
      "Prev Loss: 19.83946418762207\n",
      "Loss: 19.83946418762207\n",
      "Step 221:\n",
      "Prev Loss: 19.721071243286133\n",
      "Loss: 19.721071243286133\n",
      "Step 222:\n",
      "Prev Loss: 19.70258140563965\n",
      "Loss: 19.70258140563965\n",
      "Step 223:\n",
      "Prev Loss: 19.627248764038086\n",
      "Loss: 19.627248764038086\n",
      "Step 224:\n",
      "Prev Loss: 19.526092529296875\n",
      "Loss: 19.526092529296875\n",
      "Step 225:\n",
      "Prev Loss: 19.451316833496094\n",
      "Loss: 19.451316833496094\n",
      "Step 226:\n",
      "Prev Loss: 19.352649688720703\n",
      "Loss: 19.352649688720703\n",
      "Step 227:\n",
      "Prev Loss: 19.242877960205078\n",
      "Loss: 19.242877960205078\n",
      "Step 228:\n",
      "Prev Loss: 19.308860778808594\n",
      "Loss: 19.308860778808594\n",
      "Step 229:\n",
      "Prev Loss: 19.18292999267578\n",
      "Loss: 19.18292999267578\n",
      "Step 230:\n",
      "Prev Loss: 19.06153106689453\n",
      "Loss: 19.06153106689453\n",
      "Step 231:\n",
      "Prev Loss: 19.038761138916016\n",
      "Loss: 19.038761138916016\n",
      "Step 232:\n",
      "Prev Loss: 19.026472091674805\n",
      "Loss: 19.026472091674805\n",
      "Step 233:\n",
      "Prev Loss: 18.865283966064453\n",
      "Loss: 18.865283966064453\n",
      "Step 234:\n",
      "Prev Loss: 18.93172264099121\n",
      "Loss: 18.93172264099121\n",
      "Step 235:\n",
      "Prev Loss: 18.846811294555664\n",
      "Loss: 18.846811294555664\n",
      "Step 236:\n",
      "Prev Loss: 18.73468017578125\n",
      "Loss: 18.73468017578125\n",
      "Step 237:\n",
      "Prev Loss: 18.676177978515625\n",
      "Loss: 18.676177978515625\n",
      "Step 238:\n",
      "Prev Loss: 18.65127182006836\n",
      "Loss: 18.65127182006836\n",
      "Step 239:\n",
      "Prev Loss: 18.543806076049805\n",
      "Loss: 18.543806076049805\n",
      "Step 240:\n",
      "Prev Loss: 18.484439849853516\n",
      "Loss: 18.484439849853516\n",
      "Step 241:\n",
      "Prev Loss: 18.442663192749023\n",
      "Loss: 18.442663192749023\n",
      "Step 242:\n",
      "Prev Loss: 18.4201602935791\n",
      "Loss: 18.4201602935791\n",
      "Step 243:\n",
      "Prev Loss: 18.306625366210938\n",
      "Loss: 18.306625366210938\n",
      "Step 244:\n",
      "Prev Loss: 18.209787368774414\n",
      "Loss: 18.209787368774414\n",
      "Step 245:\n",
      "Prev Loss: 18.13150978088379\n",
      "Loss: 18.13150978088379\n",
      "Step 246:\n",
      "Prev Loss: 18.096527099609375\n",
      "Loss: 18.096527099609375\n",
      "Step 247:\n",
      "Prev Loss: 18.085044860839844\n",
      "Loss: 18.085044860839844\n",
      "Step 248:\n",
      "Prev Loss: 17.975719451904297\n",
      "Loss: 17.975719451904297\n",
      "Step 249:\n",
      "Prev Loss: 17.970123291015625\n",
      "Loss: 17.970123291015625\n",
      "Step 250:\n",
      "Prev Loss: 17.858993530273438\n",
      "Loss: 17.858993530273438\n",
      "Step 251:\n",
      "Prev Loss: 19.185049057006836\n",
      "Loss: 19.185049057006836\n",
      "Step 252:\n",
      "Prev Loss: 19.265872955322266\n",
      "Loss: 19.265872955322266\n",
      "Step 253:\n",
      "Prev Loss: 19.376556396484375\n",
      "Loss: 19.376556396484375\n",
      "Step 254:\n",
      "Prev Loss: 18.41689682006836\n",
      "Loss: 18.41689682006836\n",
      "Step 255:\n",
      "Prev Loss: 18.279399871826172\n",
      "Loss: 18.279399871826172\n",
      "Step 256:\n",
      "Prev Loss: 17.91922950744629\n",
      "Loss: 17.91922950744629\n",
      "Step 257:\n",
      "Prev Loss: 17.842208862304688\n",
      "Loss: 17.842208862304688\n",
      "Step 258:\n",
      "Prev Loss: 17.733510971069336\n",
      "Loss: 17.733510971069336\n",
      "Step 259:\n",
      "Prev Loss: 17.591796875\n",
      "Loss: 17.591796875\n",
      "Step 260:\n",
      "Prev Loss: 17.499736785888672\n",
      "Loss: 17.499736785888672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 261:\n",
      "Prev Loss: 17.427690505981445\n",
      "Loss: 17.427690505981445\n",
      "Step 262:\n",
      "Prev Loss: 17.302349090576172\n",
      "Loss: 17.302349090576172\n",
      "Step 263:\n",
      "Prev Loss: 17.22933578491211\n",
      "Loss: 17.22933578491211\n",
      "Step 264:\n",
      "Prev Loss: 17.21391487121582\n",
      "Loss: 17.21391487121582\n",
      "Step 265:\n",
      "Prev Loss: 17.155960083007812\n",
      "Loss: 17.155960083007812\n",
      "Step 266:\n",
      "Prev Loss: 17.048851013183594\n",
      "Loss: 17.048851013183594\n",
      "Step 267:\n",
      "Prev Loss: 16.997447967529297\n",
      "Loss: 16.997447967529297\n",
      "Step 268:\n",
      "Prev Loss: 17.00895881652832\n",
      "Loss: 17.00895881652832\n",
      "Step 269:\n",
      "Prev Loss: 16.903108596801758\n",
      "Loss: 16.903108596801758\n",
      "Step 270:\n",
      "Prev Loss: 16.82123374938965\n",
      "Loss: 16.82123374938965\n",
      "Step 271:\n",
      "Prev Loss: 16.827489852905273\n",
      "Loss: 16.827489852905273\n",
      "Step 272:\n",
      "Prev Loss: 17.660165786743164\n",
      "Loss: 17.660165786743164\n",
      "Step 273:\n",
      "Prev Loss: 17.272127151489258\n",
      "Loss: 17.272127151489258\n",
      "Step 274:\n",
      "Prev Loss: 21.543384552001953\n",
      "Loss: 21.543384552001953\n",
      "Step 275:\n",
      "Prev Loss: 18.98182487487793\n",
      "Loss: 18.98182487487793\n",
      "Step 276:\n",
      "Prev Loss: 18.760671615600586\n",
      "Loss: 18.760671615600586\n",
      "Step 277:\n",
      "Prev Loss: 18.566051483154297\n",
      "Loss: 18.566051483154297\n",
      "Step 278:\n",
      "Prev Loss: 17.783382415771484\n",
      "Loss: 17.783382415771484\n",
      "Step 279:\n",
      "Prev Loss: 17.588327407836914\n",
      "Loss: 17.588327407836914\n",
      "Step 280:\n",
      "Prev Loss: 17.33435821533203\n",
      "Loss: 17.33435821533203\n",
      "Step 281:\n",
      "Prev Loss: 17.095294952392578\n",
      "Loss: 17.095294952392578\n",
      "Step 282:\n",
      "Prev Loss: 16.879364013671875\n",
      "Loss: 16.879364013671875\n",
      "Step 283:\n",
      "Prev Loss: 16.724639892578125\n",
      "Loss: 16.724639892578125\n",
      "Step 284:\n",
      "Prev Loss: 16.60753631591797\n",
      "Loss: 16.60753631591797\n",
      "Step 285:\n",
      "Prev Loss: 16.498613357543945\n",
      "Loss: 16.498613357543945\n",
      "Step 286:\n",
      "Prev Loss: 16.424358367919922\n",
      "Loss: 16.424358367919922\n",
      "Step 287:\n",
      "Prev Loss: 16.350318908691406\n",
      "Loss: 16.350318908691406\n",
      "Step 288:\n",
      "Prev Loss: 16.273723602294922\n",
      "Loss: 16.273723602294922\n",
      "Step 289:\n",
      "Prev Loss: 16.17841911315918\n",
      "Loss: 16.17841911315918\n",
      "Step 290:\n",
      "Prev Loss: 16.09737205505371\n",
      "Loss: 16.09737205505371\n",
      "Step 291:\n",
      "Prev Loss: 16.02045249938965\n",
      "Loss: 16.02045249938965\n",
      "Step 292:\n",
      "Prev Loss: 15.966765403747559\n",
      "Loss: 15.966765403747559\n",
      "Step 293:\n",
      "Prev Loss: 15.978124618530273\n",
      "Loss: 15.978124618530273\n",
      "Step 294:\n",
      "Prev Loss: 15.912802696228027\n",
      "Loss: 15.912802696228027\n",
      "Step 295:\n",
      "Prev Loss: 15.835718154907227\n",
      "Loss: 15.835718154907227\n",
      "Step 296:\n",
      "Prev Loss: 15.785604476928711\n",
      "Loss: 15.785604476928711\n",
      "Step 297:\n",
      "Prev Loss: 15.783997535705566\n",
      "Loss: 15.783997535705566\n",
      "Step 298:\n",
      "Prev Loss: 15.883111953735352\n",
      "Loss: 15.883111953735352\n",
      "Step 299:\n",
      "Prev Loss: 15.875036239624023\n",
      "Loss: 15.875036239624023\n",
      "Step 300:\n",
      "Prev Loss: 15.759464263916016\n",
      "Loss: 15.759464263916016\n",
      "Step 301:\n",
      "Prev Loss: 15.689369201660156\n",
      "Loss: 15.689369201660156\n",
      "Step 302:\n",
      "Prev Loss: 15.726289749145508\n",
      "Loss: 15.726289749145508\n",
      "Step 303:\n",
      "Prev Loss: 15.705495834350586\n",
      "Loss: 15.705495834350586\n",
      "Step 304:\n",
      "Prev Loss: 15.748639106750488\n",
      "Loss: 15.748639106750488\n",
      "Step 305:\n",
      "Prev Loss: 15.711723327636719\n",
      "Loss: 15.711723327636719\n",
      "Step 306:\n",
      "Prev Loss: 15.615668296813965\n",
      "Loss: 15.615668296813965\n",
      "Step 307:\n",
      "Prev Loss: 17.533916473388672\n",
      "Loss: 17.533916473388672\n",
      "Step 308:\n",
      "Prev Loss: 18.685712814331055\n",
      "Loss: 18.685712814331055\n",
      "Step 309:\n",
      "Prev Loss: 16.97865867614746\n",
      "Loss: 16.97865867614746\n",
      "Step 310:\n",
      "Prev Loss: 16.201263427734375\n",
      "Loss: 16.201263427734375\n",
      "Step 311:\n",
      "Prev Loss: 16.334518432617188\n",
      "Loss: 16.334518432617188\n",
      "Step 312:\n",
      "Prev Loss: 15.804672241210938\n",
      "Loss: 15.804672241210938\n",
      "Step 313:\n",
      "Prev Loss: 15.717432022094727\n",
      "Loss: 15.717432022094727\n",
      "Step 314:\n",
      "Prev Loss: 15.623435020446777\n",
      "Loss: 15.623435020446777\n",
      "Step 315:\n",
      "Prev Loss: 15.517129898071289\n",
      "Loss: 15.517129898071289\n",
      "Step 316:\n",
      "Prev Loss: 15.402620315551758\n",
      "Loss: 15.402620315551758\n",
      "Step 317:\n",
      "Prev Loss: 15.312803268432617\n",
      "Loss: 15.312803268432617\n",
      "Step 318:\n",
      "Prev Loss: 15.242593765258789\n",
      "Loss: 15.242593765258789\n",
      "Step 319:\n",
      "Prev Loss: 15.193390846252441\n",
      "Loss: 15.193390846252441\n",
      "Step 320:\n",
      "Prev Loss: 15.13977336883545\n",
      "Loss: 15.13977336883545\n",
      "Step 321:\n",
      "Prev Loss: 15.097146987915039\n",
      "Loss: 15.097146987915039\n",
      "Step 322:\n",
      "Prev Loss: 15.159797668457031\n",
      "Loss: 15.159797668457031\n",
      "Step 323:\n",
      "Prev Loss: 15.088045120239258\n",
      "Loss: 15.088045120239258\n",
      "Step 324:\n",
      "Prev Loss: 15.01500415802002\n",
      "Loss: 15.01500415802002\n",
      "Step 325:\n",
      "Prev Loss: 14.986287117004395\n",
      "Loss: 14.986287117004395\n",
      "Step 326:\n",
      "Prev Loss: 15.27423095703125\n",
      "Loss: 15.27423095703125\n",
      "Step 327:\n",
      "Prev Loss: 15.547382354736328\n",
      "Loss: 15.547382354736328\n",
      "Step 328:\n",
      "Prev Loss: 15.332382202148438\n",
      "Loss: 15.332382202148438\n",
      "Step 329:\n",
      "Prev Loss: 15.148552894592285\n",
      "Loss: 15.148552894592285\n",
      "Step 330:\n",
      "Prev Loss: 16.286300659179688\n",
      "Loss: 16.286300659179688\n",
      "Step 331:\n",
      "Prev Loss: 16.305931091308594\n",
      "Loss: 16.305931091308594\n",
      "Step 332:\n",
      "Prev Loss: 16.827598571777344\n",
      "Loss: 16.827598571777344\n",
      "Step 333:\n",
      "Prev Loss: 15.970423698425293\n",
      "Loss: 15.970423698425293\n",
      "Step 334:\n",
      "Prev Loss: 15.59794807434082\n",
      "Loss: 15.59794807434082\n",
      "Step 335:\n",
      "Prev Loss: 15.324356079101562\n",
      "Loss: 15.324356079101562\n",
      "Step 336:\n",
      "Prev Loss: 15.221498489379883\n",
      "Loss: 15.221498489379883\n",
      "Step 337:\n",
      "Prev Loss: 15.094078063964844\n",
      "Loss: 15.094078063964844\n",
      "Step 338:\n",
      "Prev Loss: 14.964601516723633\n",
      "Loss: 14.964601516723633\n",
      "Step 339:\n",
      "Prev Loss: 14.861194610595703\n",
      "Loss: 14.861194610595703\n",
      "Step 340:\n",
      "Prev Loss: 14.781643867492676\n",
      "Loss: 14.781643867492676\n",
      "Step 341:\n",
      "Prev Loss: 14.728446960449219\n",
      "Loss: 14.728446960449219\n",
      "Step 342:\n",
      "Prev Loss: 14.661764144897461\n",
      "Loss: 14.661764144897461\n",
      "Step 343:\n",
      "Prev Loss: 14.597496032714844\n",
      "Loss: 14.597496032714844\n",
      "Step 344:\n",
      "Prev Loss: 14.573997497558594\n",
      "Loss: 14.573997497558594\n",
      "Step 345:\n",
      "Prev Loss: 14.553571701049805\n",
      "Loss: 14.553571701049805\n",
      "Step 346:\n",
      "Prev Loss: 14.504547119140625\n",
      "Loss: 14.504547119140625\n",
      "Step 347:\n",
      "Prev Loss: 14.491430282592773\n",
      "Loss: 14.491430282592773\n",
      "Step 348:\n",
      "Prev Loss: 14.558202743530273\n",
      "Loss: 14.558202743530273\n",
      "Step 349:\n",
      "Prev Loss: 14.55140495300293\n",
      "Loss: 14.55140495300293\n",
      "Step 350:\n",
      "Prev Loss: 14.462488174438477\n",
      "Loss: 14.462488174438477\n",
      "Step 351:\n",
      "Prev Loss: 14.436494827270508\n",
      "Loss: 14.436494827270508\n",
      "Step 352:\n",
      "Prev Loss: 14.604927062988281\n",
      "Loss: 14.604927062988281\n",
      "Step 353:\n",
      "Prev Loss: 14.610280990600586\n",
      "Loss: 14.610280990600586\n",
      "Step 354:\n",
      "Prev Loss: 14.492424964904785\n",
      "Loss: 14.492424964904785\n",
      "Step 355:\n",
      "Prev Loss: 14.525140762329102\n",
      "Loss: 14.525140762329102\n",
      "Step 356:\n",
      "Prev Loss: 14.483576774597168\n",
      "Loss: 14.483576774597168\n",
      "Step 357:\n",
      "Prev Loss: 14.404247283935547\n",
      "Loss: 14.404247283935547\n",
      "Step 358:\n",
      "Prev Loss: 14.422178268432617\n",
      "Loss: 14.422178268432617\n",
      "Step 359:\n",
      "Prev Loss: 14.495946884155273\n",
      "Loss: 14.495946884155273\n",
      "Step 360:\n",
      "Prev Loss: 14.626344680786133\n",
      "Loss: 14.626344680786133\n",
      "Step 361:\n",
      "Prev Loss: 14.573332786560059\n",
      "Loss: 14.573332786560059\n",
      "Step 362:\n",
      "Prev Loss: 18.71584129333496\n",
      "Loss: 18.71584129333496\n",
      "Step 363:\n",
      "Prev Loss: 21.328121185302734\n",
      "Loss: 21.328121185302734\n",
      "Step 364:\n",
      "Prev Loss: 19.209064483642578\n",
      "Loss: 19.209064483642578\n",
      "Step 365:\n",
      "Prev Loss: 17.301910400390625\n",
      "Loss: 17.301910400390625\n",
      "Step 366:\n",
      "Prev Loss: 16.82815933227539\n",
      "Loss: 16.82815933227539\n",
      "Step 367:\n",
      "Prev Loss: 15.870068550109863\n",
      "Loss: 15.870068550109863\n",
      "Step 368:\n",
      "Prev Loss: 15.543059349060059\n",
      "Loss: 15.543059349060059\n",
      "Step 369:\n",
      "Prev Loss: 15.30120849609375\n",
      "Loss: 15.30120849609375\n",
      "Step 370:\n",
      "Prev Loss: 15.081689834594727\n",
      "Loss: 15.081689834594727\n",
      "Step 371:\n",
      "Prev Loss: 14.889293670654297\n",
      "Loss: 14.889293670654297\n",
      "Step 372:\n",
      "Prev Loss: 14.72593879699707\n",
      "Loss: 14.72593879699707\n",
      "Step 373:\n",
      "Prev Loss: 14.589683532714844\n",
      "Loss: 14.589683532714844\n",
      "Step 374:\n",
      "Prev Loss: 14.479510307312012\n",
      "Loss: 14.479510307312012\n",
      "Step 375:\n",
      "Prev Loss: 14.399199485778809\n",
      "Loss: 14.399199485778809\n",
      "Step 376:\n",
      "Prev Loss: 14.326661109924316\n",
      "Loss: 14.326661109924316\n",
      "Step 377:\n",
      "Prev Loss: 14.28931999206543\n",
      "Loss: 14.28931999206543\n",
      "Step 378:\n",
      "Prev Loss: 14.229738235473633\n",
      "Loss: 14.229738235473633\n",
      "Step 379:\n",
      "Prev Loss: 14.184337615966797\n",
      "Loss: 14.184337615966797\n",
      "Step 380:\n",
      "Prev Loss: 14.13809585571289\n",
      "Loss: 14.13809585571289\n",
      "Step 381:\n",
      "Prev Loss: 14.096923828125\n",
      "Loss: 14.096923828125\n",
      "Step 382:\n",
      "Prev Loss: 14.043411254882812\n",
      "Loss: 14.043411254882812\n",
      "Step 383:\n",
      "Prev Loss: 14.005815505981445\n",
      "Loss: 14.005815505981445\n",
      "Step 384:\n",
      "Prev Loss: 13.97530460357666\n",
      "Loss: 13.97530460357666\n",
      "Step 385:\n",
      "Prev Loss: 13.950053215026855\n",
      "Loss: 13.950053215026855\n",
      "Step 386:\n",
      "Prev Loss: 13.921497344970703\n",
      "Loss: 13.921497344970703\n",
      "Step 387:\n",
      "Prev Loss: 13.88912582397461\n",
      "Loss: 13.88912582397461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 388:\n",
      "Prev Loss: 13.865180969238281\n",
      "Loss: 13.865180969238281\n",
      "Step 389:\n",
      "Prev Loss: 13.850677490234375\n",
      "Loss: 13.850677490234375\n",
      "Step 390:\n",
      "Prev Loss: 13.845449447631836\n",
      "Loss: 13.845449447631836\n",
      "Step 391:\n",
      "Prev Loss: 13.836726188659668\n",
      "Loss: 13.836726188659668\n",
      "Step 392:\n",
      "Prev Loss: 13.832921028137207\n",
      "Loss: 13.832921028137207\n",
      "Step 393:\n",
      "Prev Loss: 13.80451774597168\n",
      "Loss: 13.80451774597168\n",
      "Step 394:\n",
      "Prev Loss: 13.847801208496094\n",
      "Loss: 13.847801208496094\n",
      "Step 395:\n",
      "Prev Loss: 13.92280387878418\n",
      "Loss: 13.92280387878418\n",
      "Step 396:\n",
      "Prev Loss: 13.832329750061035\n",
      "Loss: 13.832329750061035\n",
      "Step 397:\n",
      "Prev Loss: 13.759674072265625\n",
      "Loss: 13.759674072265625\n",
      "Step 398:\n",
      "Prev Loss: 14.020036697387695\n",
      "Loss: 14.020036697387695\n",
      "Step 399:\n",
      "Prev Loss: 13.8256254196167\n",
      "Loss: 13.8256254196167\n",
      "Step 400:\n",
      "Prev Loss: 13.777727127075195\n",
      "Loss: 13.777727127075195\n",
      "Step 401:\n",
      "Prev Loss: 14.512786865234375\n",
      "Loss: 14.512786865234375\n",
      "Step 402:\n",
      "Prev Loss: 14.274345397949219\n",
      "Loss: 14.274345397949219\n",
      "Step 403:\n",
      "Prev Loss: 14.410927772521973\n",
      "Loss: 14.410927772521973\n",
      "Step 404:\n",
      "Prev Loss: 14.25767707824707\n",
      "Loss: 14.25767707824707\n",
      "Step 405:\n",
      "Prev Loss: 14.090690612792969\n",
      "Loss: 14.090690612792969\n",
      "Step 406:\n",
      "Prev Loss: 13.969722747802734\n",
      "Loss: 13.969722747802734\n",
      "Step 407:\n",
      "Prev Loss: 13.899696350097656\n",
      "Loss: 13.899696350097656\n",
      "Step 408:\n",
      "Prev Loss: 13.854622840881348\n",
      "Loss: 13.854622840881348\n",
      "Step 409:\n",
      "Prev Loss: 13.81112003326416\n",
      "Loss: 13.81112003326416\n",
      "Step 410:\n",
      "Prev Loss: 13.731210708618164\n",
      "Loss: 13.731210708618164\n",
      "Step 411:\n",
      "Prev Loss: 13.685009002685547\n",
      "Loss: 13.685009002685547\n",
      "Step 412:\n",
      "Prev Loss: 13.67192268371582\n",
      "Loss: 13.67192268371582\n",
      "Step 413:\n",
      "Prev Loss: 13.599787712097168\n",
      "Loss: 13.599787712097168\n",
      "Step 414:\n",
      "Prev Loss: 13.579702377319336\n",
      "Loss: 13.579702377319336\n",
      "Step 415:\n",
      "Prev Loss: 13.69759464263916\n",
      "Loss: 13.69759464263916\n",
      "Step 416:\n",
      "Prev Loss: 13.579051971435547\n",
      "Loss: 13.579051971435547\n",
      "Step 417:\n",
      "Prev Loss: 13.548920631408691\n",
      "Loss: 13.548920631408691\n",
      "Step 418:\n",
      "Prev Loss: 13.551074981689453\n",
      "Loss: 13.551074981689453\n",
      "Step 419:\n",
      "Prev Loss: 13.816864013671875\n",
      "Loss: 13.816864013671875\n",
      "Step 420:\n",
      "Prev Loss: 13.738153457641602\n",
      "Loss: 13.738153457641602\n",
      "Step 421:\n",
      "Prev Loss: 13.773117065429688\n",
      "Loss: 13.773117065429688\n",
      "Step 422:\n",
      "Prev Loss: 18.408126831054688\n",
      "Loss: 18.408126831054688\n",
      "Step 423:\n",
      "Prev Loss: 16.580896377563477\n",
      "Loss: 16.580896377563477\n",
      "Step 424:\n",
      "Prev Loss: 14.827327728271484\n",
      "Loss: 14.827327728271484\n",
      "Step 425:\n",
      "Prev Loss: 14.555550575256348\n",
      "Loss: 14.555550575256348\n",
      "Step 426:\n",
      "Prev Loss: 14.189336776733398\n",
      "Loss: 14.189336776733398\n",
      "Step 427:\n",
      "Prev Loss: 14.070167541503906\n",
      "Loss: 14.070167541503906\n",
      "Step 428:\n",
      "Prev Loss: 13.95053768157959\n",
      "Loss: 13.95053768157959\n",
      "Step 429:\n",
      "Prev Loss: 13.846622467041016\n",
      "Loss: 13.846622467041016\n",
      "Step 430:\n",
      "Prev Loss: 13.750917434692383\n",
      "Loss: 13.750917434692383\n",
      "Step 431:\n",
      "Prev Loss: 13.706284523010254\n",
      "Loss: 13.706284523010254\n",
      "Step 432:\n",
      "Prev Loss: 13.645414352416992\n",
      "Loss: 13.645414352416992\n",
      "Step 433:\n",
      "Prev Loss: 13.5856294631958\n",
      "Loss: 13.5856294631958\n",
      "Step 434:\n",
      "Prev Loss: 13.534191131591797\n",
      "Loss: 13.534191131591797\n",
      "Step 435:\n",
      "Prev Loss: 13.492027282714844\n",
      "Loss: 13.492027282714844\n",
      "Step 436:\n",
      "Prev Loss: 13.452630043029785\n",
      "Loss: 13.452630043029785\n",
      "Step 437:\n",
      "Prev Loss: 13.425614356994629\n",
      "Loss: 13.425614356994629\n",
      "Step 438:\n",
      "Prev Loss: 13.404341697692871\n",
      "Loss: 13.404341697692871\n",
      "Step 439:\n",
      "Prev Loss: 13.385130882263184\n",
      "Loss: 13.385130882263184\n",
      "Step 440:\n",
      "Prev Loss: 13.368708610534668\n",
      "Loss: 13.368708610534668\n",
      "Step 441:\n",
      "Prev Loss: 13.363924980163574\n",
      "Loss: 13.363924980163574\n",
      "Step 442:\n",
      "Prev Loss: 13.368316650390625\n",
      "Loss: 13.368316650390625\n",
      "Step 443:\n",
      "Prev Loss: 13.383739471435547\n",
      "Loss: 13.383739471435547\n",
      "Step 444:\n",
      "Prev Loss: 13.391887664794922\n",
      "Loss: 13.391887664794922\n",
      "Step 445:\n",
      "Prev Loss: 13.393824577331543\n",
      "Loss: 13.393824577331543\n",
      "Step 446:\n",
      "Prev Loss: 13.409358024597168\n",
      "Loss: 13.409358024597168\n",
      "Step 447:\n",
      "Prev Loss: 13.49124526977539\n",
      "Loss: 13.49124526977539\n",
      "Step 448:\n",
      "Prev Loss: 13.510579109191895\n",
      "Loss: 13.510579109191895\n",
      "Step 449:\n",
      "Prev Loss: 13.437751770019531\n",
      "Loss: 13.437751770019531\n",
      "Step 450:\n",
      "Prev Loss: 13.409382820129395\n",
      "Loss: 13.409382820129395\n",
      "Step 451:\n",
      "Prev Loss: 13.4236421585083\n",
      "Loss: 13.4236421585083\n",
      "Step 452:\n",
      "Prev Loss: 13.656023979187012\n",
      "Loss: 13.656023979187012\n",
      "Step 453:\n",
      "Prev Loss: 13.74019718170166\n",
      "Loss: 13.74019718170166\n",
      "Step 454:\n",
      "Prev Loss: 13.576432228088379\n",
      "Loss: 13.576432228088379\n",
      "Step 455:\n",
      "Prev Loss: 14.106943130493164\n",
      "Loss: 14.106943130493164\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.LBFGS([input_img])\n",
    "style_weight = 1e5\n",
    "content_weight = 1.5\n",
    "\n",
    "steps = 0\n",
    "prev_loss = 0\n",
    "\n",
    "content_loss_w = 0\n",
    "style_loss_loss_w = 0\n",
    "\n",
    "while steps <= 1000 and prev_loss < 100:\n",
    "    def closure():\n",
    "        with torch.no_grad():\n",
    "            input_img.clamp_(0, 1)\n",
    "        global steps\n",
    "        global prev_loss\n",
    "        optimizer.zero_grad()\n",
    "        model(input_img)\n",
    "        content_loss = 0\n",
    "        style_loss = 0\n",
    "        for l in content_losses:\n",
    "            content_loss += l.loss\n",
    "        for l in style_losses:\n",
    "            style_loss += l.loss\n",
    "        loss = content_weight * content_loss + style_weight * style_loss\n",
    "        loss.backward()\n",
    "        steps += 1\n",
    "        if steps % 1 == 0:\n",
    "            print(f'Step {steps}:')\n",
    "            print(f'Prev Loss: {loss}')\n",
    "            print(f'Loss: {loss}')\n",
    "            save_image(input_img, f'images/output_{steps}.jpg')\n",
    "        prev_loss = loss\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    input_img.clamp_(0, 1)\n",
    "    \n",
    "save_image(input_img, 'images/output_final.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4835513d",
   "metadata": {},
   "source": [
    "And we can obtain a baby grogu image that is the same style as a [Andy Warhol](https://en.wikipedia.org/wiki/Andy_Warhol) Painting:\n",
    "\n",
    "\n",
    "Style Image                   | Transfer Image   | Content Image   \n",
    ":-------------------------:|:-------------------------:|:-------------------------:\n",
    "<img src=\"images/andyw.jpg\"  width=\"256\" height=\"256\">  |  ![](images/output_700.jpg)  | <img src=\"images/original.jpeg\"  width=\"256\" height=\"256\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c8edb0",
   "metadata": {},
   "source": [
    "## 5. AI Distributed Training with Determined AI\n",
    "\n",
    "Next, we use [determined](https://github.com/determined-ai/determined) package to enable distributed model training across clusters for more efficient training process and performance optimization. \n",
    "\n",
    "There are two ways I tried to initialize a determined AI project: WEB cluster interface vs Docker. The determined Version used in this project is:\n",
    "```shell\n",
    "$ det --version\n",
    "det 0.21.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9b360c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'determined'...\n",
      "remote: Enumerating objects: 126324, done.\u001b[K\n",
      "remote: Counting objects: 100% (1477/1477), done.\u001b[K\n",
      "remote: Compressing objects: 100% (740/740), done.\u001b[K\n",
      "remote: Total 126324 (delta 889), reused 1183 (delta 707), pack-reused 124847\u001b[K\n",
      "Receiving objects: 100% (126324/126324), 121.64 MiB | 29.15 MiB/s, done.\n",
      "Resolving deltas: 100% (97207/97207), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/determined-ai/determined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612b298",
   "metadata": {},
   "source": [
    "Following we will use **PyTorchTrial** based on [Determined AI PyTorch Trail](https://docs.determined.ai/latest/training/apis-howto/api-pytorch-ug.html) for specifying experiment configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b29c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union, Sequence\n",
    "from determined.pytorch import DataLoader, PyTorchTrial, PyTorchTrialContext\n",
    "import os\n",
    "\n",
    "TorchData = Union[Dict[str, torch.Tensor], Sequence[torch.Tensor], torch.Tensor]\n",
    "\n",
    "class MyTrial(PyTorchTrial):\n",
    "    def __init__(self, context: PyTorchTrialContext) -> None:\n",
    "        self.context = context\n",
    "        self.optimizer = optim.LBFGS([input_img])\n",
    "        \n",
    "    def build_model(self):\n",
    "        model, content_losses, style_losses = get_model_and_losses(\n",
    "    content_img, style_img, default_content_layers, default_style_layers)\n",
    "        return model\n",
    "    \n",
    "    def build_training_data_loader(self) -> DataLoader:\n",
    "        traindir = os.path.join(os.getcwd(), 'images')\n",
    "        self.normalize = Normalization()\n",
    "\n",
    "        train_dataset = datasets.ImageFolder(\n",
    "            traindir,\n",
    "            transforms.Compose([\n",
    "                transforms.RandomResizedCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                self.normalize,\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        train_loader = determined.pytorch.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.context.get_per_slot_batch_size(),\n",
    "            shuffle=True,\n",
    "            num_workers=self.context.get_hparam(\"workers\", pin_memory=True),\n",
    "        )\n",
    "        \n",
    "        return train_loader\n",
    "\n",
    "    def build_validation_data_loader(self) -> DataLoader:\n",
    "        return DataLoader()\n",
    "\n",
    "    def train_batch(self, batch: TorchData, epoch_idx: int, batch_idx: int)  -> Dict[str, Any]:\n",
    "        return {}\n",
    "\n",
    "    def evaluate_batch(self, batch: TorchData) -> Dict[str, Any]:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd0ec564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    with det.pytorch.init() as train_context:\n",
    "        trial = MyTrial(train_context)\n",
    "        trainer = det.pytorch.Trainer(trial, train_context)\n",
    "        trainer.fit(\n",
    "            max_length=pytorch.Epoch(1),\n",
    "            checkpoint_period=pytorch.Batch(500),\n",
    "            validation_period=pytorch.Batch(500),\n",
    "            checkpoint_policy=\"all\",\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO, format=det.LOG_FORMAT)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01c1664",
   "metadata": {},
   "source": [
    "Use **local distributed training**, function initialization becomes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "89b6523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dist.init_process_group(backend=\"gloo|nccl\")  \n",
    "    os.environ[\"USE_TORCH_DISTRIBUTED\"] = \"true\"\n",
    "    \n",
    "    with det.pytorch.init(distributed=core.DistributedContext.from_torch_distributed()) as train_context:\n",
    "        trial = MyTrial(train_context)\n",
    "        trainer = det.pytorch.Trainer(trial, train_context)\n",
    "        trainer.fit(\n",
    "            max_length=pytorch.Epoch(1),\n",
    "            checkpoint_period=pytorch.Batch(100),\n",
    "            validation_period=pytorch.Batch(100),\n",
    "            checkpoint_policy=\"all\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c364ee4",
   "metadata": {},
   "source": [
    "To train the model, whether it's single instance or distributed, we can start using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a037467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model=[\"name:image_style_transfer\",\n",
    "              \"global_batch_size: 32\",\n",
    "               \"dense1: 128\",\n",
    "               \"name: single\",\n",
    "               \"metric: val_accuracy\",\n",
    "               \"epochs: 5\",\n",
    "               \"entrypoint: model_def:image_style_transfer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e4ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7aaefc1",
   "metadata": {},
   "source": [
    "## 6. References\n",
    "\n",
    "- [PyTorch](https://pytorch.org/) including [TorchVision](https://pytorch.org/vision/)\n",
    "- [Basic Setup - Determined AI Documentation](https://docs.determined.ai/latest/cluster-setup-guide/basic.html)\n",
    "- [Install Determined Using Docker - DEtermined AI Documentation](https://www.googleadservices.com/pagead/aclk?sa=L&ai=DChcSEwjr-L31k7D-AhVFLK0GHcQMDlYYABAAGgJwdg&ohost=www.google.com&cid=CAESauD2a8aozeJBdbuvL_0G3p8oPtY-Z57CWfOI519uSb4QLuOJapvAsKL-hTA_kYiMZENmLoC-ZB4b7Y-QwvCWwrdlsfCE-nW2ATpYJhG8Id7saqRay842BZHpfXkN7T7PpmLaVUa0OYGjn9c&sig=AOD64_0HSgtNM8ymy1FKpSbmmeQorcDIdw&q&adurl&ved=2ahUKEwjWnrb1k7D-AhVQhu4BHciYBWkQ0Qx6BAgHEAE)\n",
    "- [Image Style Transfer Using Convolutional Neural Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)\n",
    "- [A Keras Implementation of Image Style Transfer Using Convolutional Neural Networks, Gatys et al](https://github.com/superb20/Image-Style-Transfer-Using-Convolutional-Neural-Networks)\n",
    "- [a PyTorch implementation of Image Style Transfer Using Convolutional Neural Networks](https://github.com/ali-gtw/ImageStyleTransfer-CNN)\n",
    "- [Image Style Transfer using CNN](https://github.com/Suvoo/Image-Style-Transfer-Using-CNNs)\n",
    "- [DL Style Transfer Demos](github.com/SingleZombie/DL-Demos/tree/master/dldemos/StyleTransfer)\n",
    "- [PyTorch Style Transfer Official Tutorials](https://link.zhihu.com/?target=https%3A//pytorch.org/tutorials/advanced/neural_style_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea9b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
